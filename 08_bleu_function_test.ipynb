{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bleu() function test.ipynb",
      "provenance": [],
      "mount_file_id": "1VPXuAPz_EaoRPdSMbF2vrFWBSQghJffY",
      "authorship_tag": "ABX9TyNUMuY+eAHy5vbJA3yrm7AD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DojunPark/Machine_Translation/blob/master/08_bleu_function_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8shFf4PRYqT",
        "outputId": "e86f6489-b152-477d-9fcc-77e1ef2b8631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Oct 24 02:40:38 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0lvKW0cRZTc",
        "outputId": "f661cec0-a4de-4e8d-da99-0f360b3f084a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 13.7 gigabytes of available RAM\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drHOjGG3SIoI"
      },
      "source": [
        "# installing the tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZdKGR8SRZWB",
        "outputId": "1ed41f58-0b6b-4cab-b9d1-40cecab0c948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "!pip install konlpy\n",
        "!sudo apt-get install curl git\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "\n",
        "print(f'mecab 설치 소요 시간: {((time.time()-start)/60):.2f} min')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.5MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 14.0MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: colorama, beautifulsoup4, tweepy, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.58.0-2ubuntu3.10).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Installing automake (A dependency for mecab-ko)\n",
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [39.3 kB]\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [370 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [57.0 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.4 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,352 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,681 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [211 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,745 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.9 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,162 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,115 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [238 kB]\n",
            "Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [860 kB]\n",
            "Fetched 11.2 MB in 3s (3,544 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  autoconf autotools-dev libsigsegv2 m4\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc libtool gettext m4-doc\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autotools-dev libsigsegv2 m4\n",
            "0 upgraded, 5 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 1,082 kB of archives.\n",
            "After this operation, 3,994 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Fetched 1,082 kB in 1s (835 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libsigsegv2:amd64.\n",
            "(Reading database ... 144611 files and directories currently installed.)\n",
            "Preparing to unpack .../libsigsegv2_2.12-1_amd64.deb ...\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\n",
            "Selecting previously unselected package m4.\n",
            "Preparing to unpack .../archives/m4_1.4.18-1_amd64.deb ...\n",
            "Unpacking m4 (1.4.18-1) ...\n",
            "Selecting previously unselected package autoconf.\n",
            "Preparing to unpack .../autoconf_2.69-11_all.deb ...\n",
            "Unpacking autoconf (2.69-11) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package automake.\n",
            "Preparing to unpack .../automake_1%3a1.15.1-3ubuntu2_all.deb ...\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\n",
            "Setting up m4 (1.4.18-1) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up autoconf (2.69-11) ...\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Install mecab-ko-dic\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1381k  100 1381k    0     0  1134k      0  0:00:01  0:00:01 --:--:-- 2268k\n",
            "mecab-0.996-ko-0.9.2/\n",
            "mecab-0.996-ko-0.9.2/example/\n",
            "mecab-0.996-ko-0.9.2/example/example.cpp\n",
            "mecab-0.996-ko-0.9.2/example/example_lattice.cpp\n",
            "mecab-0.996-ko-0.9.2/example/example_lattice.c\n",
            "mecab-0.996-ko-0.9.2/example/example.c\n",
            "mecab-0.996-ko-0.9.2/example/thread_test.cpp\n",
            "mecab-0.996-ko-0.9.2/mecab-config.in\n",
            "mecab-0.996-ko-0.9.2/man/\n",
            "mecab-0.996-ko-0.9.2/man/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/man/mecab.1\n",
            "mecab-0.996-ko-0.9.2/man/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/mecab.iss.in\n",
            "mecab-0.996-ko-0.9.2/config.guess\n",
            "mecab-0.996-ko-0.9.2/README\n",
            "mecab-0.996-ko-0.9.2/COPYING\n",
            "mecab-0.996-ko-0.9.2/CHANGES.md\n",
            "mecab-0.996-ko-0.9.2/README.md\n",
            "mecab-0.996-ko-0.9.2/INSTALL\n",
            "mecab-0.996-ko-0.9.2/config.sub\n",
            "mecab-0.996-ko-0.9.2/configure.in\n",
            "mecab-0.996-ko-0.9.2/swig/\n",
            "mecab-0.996-ko-0.9.2/swig/Makefile\n",
            "mecab-0.996-ko-0.9.2/swig/version.h.in\n",
            "mecab-0.996-ko-0.9.2/swig/version.h\n",
            "mecab-0.996-ko-0.9.2/swig/MeCab.i\n",
            "mecab-0.996-ko-0.9.2/aclocal.m4\n",
            "mecab-0.996-ko-0.9.2/LGPL\n",
            "mecab-0.996-ko-0.9.2/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/configure\n",
            "mecab-0.996-ko-0.9.2/tests/\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/test\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/autolink/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/t9/\n",
            "mecab-0.996-ko-0.9.2/tests/t9/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/t9/ipadic.pl\n",
            "mecab-0.996-ko-0.9.2/tests/t9/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/t9/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/t9/test\n",
            "mecab-0.996-ko-0.9.2/tests/t9/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/t9/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/t9/mkdic.pl\n",
            "mecab-0.996-ko-0.9.2/tests/t9/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/ipa.train\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/ipa.test\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/rewrite.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/feature.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/cost-train/seed/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/run-eval.sh\n",
            "mecab-0.996-ko-0.9.2/tests/run-cost-train.sh\n",
            "mecab-0.996-ko-0.9.2/tests/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/test\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/katakana/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/eval/\n",
            "mecab-0.996-ko-0.9.2/tests/eval/answer\n",
            "mecab-0.996-ko-0.9.2/tests/eval/system\n",
            "mecab-0.996-ko-0.9.2/tests/eval/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/test\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/mkdic.pl\n",
            "mecab-0.996-ko-0.9.2/tests/shiin/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/latin/\n",
            "mecab-0.996-ko-0.9.2/tests/latin/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/latin/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/latin/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/latin/test\n",
            "mecab-0.996-ko-0.9.2/tests/latin/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/latin/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/latin/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/test\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/chartype/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/run-dics.sh\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/unk.def\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/dicrc\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/dic.csv\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/test\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/char.def\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/matrix.def\n",
            "mecab-0.996-ko-0.9.2/tests/ngram/test.gld\n",
            "mecab-0.996-ko-0.9.2/tests/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/ltmain.sh\n",
            "mecab-0.996-ko-0.9.2/config.rpath\n",
            "mecab-0.996-ko-0.9.2/config.h.in\n",
            "mecab-0.996-ko-0.9.2/mecabrc.in\n",
            "mecab-0.996-ko-0.9.2/GPL\n",
            "mecab-0.996-ko-0.9.2/Makefile.train\n",
            "mecab-0.996-ko-0.9.2/ChangeLog\n",
            "mecab-0.996-ko-0.9.2/install-sh\n",
            "mecab-0.996-ko-0.9.2/AUTHORS\n",
            "mecab-0.996-ko-0.9.2/doc/\n",
            "mecab-0.996-ko-0.9.2/doc/bindings.html\n",
            "mecab-0.996-ko-0.9.2/doc/posid.html\n",
            "mecab-0.996-ko-0.9.2/doc/unk.html\n",
            "mecab-0.996-ko-0.9.2/doc/learn.html\n",
            "mecab-0.996-ko-0.9.2/doc/format.html\n",
            "mecab-0.996-ko-0.9.2/doc/libmecab.html\n",
            "mecab-0.996-ko-0.9.2/doc/mecab.css\n",
            "mecab-0.996-ko-0.9.2/doc/feature.html\n",
            "mecab-0.996-ko-0.9.2/doc/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/doc/soft.html\n",
            "mecab-0.996-ko-0.9.2/doc/en/\n",
            "mecab-0.996-ko-0.9.2/doc/en/bindings.html\n",
            "mecab-0.996-ko-0.9.2/doc/dic-detail.html\n",
            "mecab-0.996-ko-0.9.2/doc/flow.png\n",
            "mecab-0.996-ko-0.9.2/doc/mecab.html\n",
            "mecab-0.996-ko-0.9.2/doc/index.html\n",
            "mecab-0.996-ko-0.9.2/doc/result.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_a.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_eval.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/functions_vars.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/doxygen.css\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_r.gif\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/functions.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Tagger.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h_source.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tabs.css\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/nav_f.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_b.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/nav_h.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_h.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_func.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/closed.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_l.gif\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/functions_func.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_type.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Lattice-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_func.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_s.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers_type.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classMeCab_1_1Model-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__dictionary__info__t.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespaces.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespacemembers.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/namespaceMeCab.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__path__t.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/files.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/structmecab__node__t-members.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/index.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/annotated.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/globals_defs.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/classes.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h-source.html\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/doxygen.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/tab_b.gif\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/bc_s.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/open.png\n",
            "mecab-0.996-ko-0.9.2/doc/doxygen/mecab_8h.html\n",
            "mecab-0.996-ko-0.9.2/doc/dic.html\n",
            "mecab-0.996-ko-0.9.2/doc/partial.html\n",
            "mecab-0.996-ko-0.9.2/doc/feature.png\n",
            "mecab-0.996-ko-0.9.2/doc/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/missing\n",
            "mecab-0.996-ko-0.9.2/BSD\n",
            "mecab-0.996-ko-0.9.2/NEWS\n",
            "mecab-0.996-ko-0.9.2/mkinstalldirs\n",
            "mecab-0.996-ko-0.9.2/src/\n",
            "mecab-0.996-ko-0.9.2/src/dictionary.h\n",
            "mecab-0.996-ko-0.9.2/src/writer.h\n",
            "mecab-0.996-ko-0.9.2/src/utils.h\n",
            "mecab-0.996-ko-0.9.2/src/string_buffer.cpp\n",
            "mecab-0.996-ko-0.9.2/src/tokenizer.cpp\n",
            "mecab-0.996-ko-0.9.2/src/make.bat\n",
            "mecab-0.996-ko-0.9.2/src/mecab.h\n",
            "mecab-0.996-ko-0.9.2/src/freelist.h\n",
            "mecab-0.996-ko-0.9.2/src/string_buffer.h\n",
            "mecab-0.996-ko-0.9.2/src/learner_tagger.h\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_compiler.cpp\n",
            "mecab-0.996-ko-0.9.2/src/eval.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-system-eval.cpp\n",
            "mecab-0.996-ko-0.9.2/src/darts.h\n",
            "mecab-0.996-ko-0.9.2/src/param.h\n",
            "mecab-0.996-ko-0.9.2/src/char_property.h\n",
            "mecab-0.996-ko-0.9.2/src/learner_node.h\n",
            "mecab-0.996-ko-0.9.2/src/mecab-dict-gen.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-dict-index.cpp\n",
            "mecab-0.996-ko-0.9.2/src/winmain.h\n",
            "mecab-0.996-ko-0.9.2/src/thread.h\n",
            "mecab-0.996-ko-0.9.2/src/context_id.cpp\n",
            "mecab-0.996-ko-0.9.2/src/Makefile.am\n",
            "mecab-0.996-ko-0.9.2/src/connector.h\n",
            "mecab-0.996-ko-0.9.2/src/common.h\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_rewriter.cpp\n",
            "mecab-0.996-ko-0.9.2/src/Makefile.msvc.in\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_rewriter.h\n",
            "mecab-0.996-ko-0.9.2/src/feature_index.h\n",
            "mecab-0.996-ko-0.9.2/src/iconv_utils.cpp\n",
            "mecab-0.996-ko-0.9.2/src/char_property.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-test-gen.cpp\n",
            "mecab-0.996-ko-0.9.2/src/tagger.cpp\n",
            "mecab-0.996-ko-0.9.2/src/mecab-cost-train.cpp\n",
            "mecab-0.996-ko-0.9.2/src/learner.cpp\n",
            "mecab-0.996-ko-0.9.2/src/dictionary.cpp\n",
            "mecab-0.996-ko-0.9.2/src/lbfgs.cpp\n",
            "mecab-0.996-ko-0.9.2/src/ucs.h\n",
            "mecab-0.996-ko-0.9.2/src/writer.cpp\n",
            "mecab-0.996-ko-0.9.2/src/learner_tagger.cpp\n",
            "mecab-0.996-ko-0.9.2/src/lbfgs.h\n",
            "mecab-0.996-ko-0.9.2/src/libmecab.cpp\n",
            "mecab-0.996-ko-0.9.2/src/tokenizer.h\n",
            "mecab-0.996-ko-0.9.2/src/mecab.cpp\n",
            "mecab-0.996-ko-0.9.2/src/utils.cpp\n",
            "mecab-0.996-ko-0.9.2/src/dictionary_generator.cpp\n",
            "mecab-0.996-ko-0.9.2/src/param.cpp\n",
            "mecab-0.996-ko-0.9.2/src/context_id.h\n",
            "mecab-0.996-ko-0.9.2/src/mmap.h\n",
            "mecab-0.996-ko-0.9.2/src/viterbi.h\n",
            "mecab-0.996-ko-0.9.2/src/viterbi.cpp\n",
            "mecab-0.996-ko-0.9.2/src/stream_wrapper.h\n",
            "mecab-0.996-ko-0.9.2/src/feature_index.cpp\n",
            "mecab-0.996-ko-0.9.2/src/nbest_generator.h\n",
            "mecab-0.996-ko-0.9.2/src/ucstable.h\n",
            "mecab-0.996-ko-0.9.2/src/nbest_generator.cpp\n",
            "mecab-0.996-ko-0.9.2/src/iconv_utils.h\n",
            "mecab-0.996-ko-0.9.2/src/connector.cpp\n",
            "mecab-0.996-ko-0.9.2/src/Makefile.in\n",
            "mecab-0.996-ko-0.9.2/src/scoped_ptr.h\n",
            "mecab-0.996-ko-0.9.2/Makefile.in\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for style of include used by make... GNU\n",
            "checking dependency style of gcc... none\n",
            "checking for g++... g++\n",
            "checking whether we are using the GNU C++ compiler... yes\n",
            "checking whether g++ accepts -g... yes\n",
            "checking dependency style of g++... none\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for egrep... /bin/grep -E\n",
            "checking whether gcc needs -traditional... no\n",
            "checking whether make sets $(MAKE)... (cached) yes\n",
            "checking build system type... x86_64-unknown-linux-gnu\n",
            "checking host system type... x86_64-unknown-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /bin/sed\n",
            "checking for fgrep... /bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking whether the shell understands some XSI constructs... yes\n",
            "checking whether the shell understands \"+=\"... yes\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... dlltool\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "./configure: line 7378: /usr/bin/file: No such file or directory\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking how to run the C++ preprocessor... g++ -E\n",
            "checking for ld used by g++... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking for g++ option to produce PIC... -fPIC -DPIC\n",
            "checking if g++ PIC flag -fPIC -DPIC works... yes\n",
            "checking if g++ static flag -static works... yes\n",
            "checking if g++ supports -c -o file.o... yes\n",
            "checking if g++ supports -c -o file.o... (cached) yes\n",
            "checking whether the g++ linker (/usr/bin/ld) supports shared libraries... yes\n",
            "checking dynamic linker characteristics... (cached) GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking for library containing strerror... none required\n",
            "checking whether byte ordering is bigendian... no\n",
            "checking for ld used by GCC... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for shared library run path origin... done\n",
            "checking for iconv... yes\n",
            "checking for working iconv... yes\n",
            "checking for iconv declaration... \n",
            "         extern size_t iconv (iconv_t cd, char * *inbuf, size_t *inbytesleft, char * *outbuf, size_t *outbytesleft);\n",
            "checking for ANSI C header files... (cached) yes\n",
            "checking for an ANSI C-conforming const... yes\n",
            "checking whether byte ordering is bigendian... (cached) no\n",
            "checking for string.h... (cached) yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking fcntl.h usability... yes\n",
            "checking fcntl.h presence... yes\n",
            "checking for fcntl.h... yes\n",
            "checking for stdint.h... (cached) yes\n",
            "checking for sys/stat.h... (cached) yes\n",
            "checking sys/mman.h usability... yes\n",
            "checking sys/mman.h presence... yes\n",
            "checking for sys/mman.h... yes\n",
            "checking sys/times.h usability... yes\n",
            "checking sys/times.h presence... yes\n",
            "checking for sys/times.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking dirent.h usability... yes\n",
            "checking dirent.h presence... yes\n",
            "checking for dirent.h... yes\n",
            "checking ctype.h usability... yes\n",
            "checking ctype.h presence... yes\n",
            "checking for ctype.h... yes\n",
            "checking for sys/types.h... (cached) yes\n",
            "checking io.h usability... no\n",
            "checking io.h presence... no\n",
            "checking for io.h... no\n",
            "checking windows.h usability... no\n",
            "checking windows.h presence... no\n",
            "checking for windows.h... no\n",
            "checking pthread.h usability... yes\n",
            "checking pthread.h presence... yes\n",
            "checking for pthread.h... yes\n",
            "checking for off_t... yes\n",
            "checking for size_t... yes\n",
            "checking size of char... 1\n",
            "checking size of short... 2\n",
            "checking size of int... 4\n",
            "checking size of long... 8\n",
            "checking size of long long... 8\n",
            "checking size of size_t... 8\n",
            "checking for size_t... (cached) yes\n",
            "checking for unsigned long long int... yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking for main in -lstdc++... yes\n",
            "checking for pthread_create in -lpthread... yes\n",
            "checking for pthread_join in -lpthread... yes\n",
            "checking for getenv... yes\n",
            "checking for opendir... yes\n",
            "checking whether make is GNU Make... yes\n",
            "checking if g++ supports stl <vector> (required)... yes\n",
            "checking if g++ supports stl <list> (required)... yes\n",
            "checking if g++ supports stl <map> (required)... yes\n",
            "checking if g++ supports stl <set> (required)... yes\n",
            "checking if g++ supports stl <queue> (required)... yes\n",
            "checking if g++ supports stl <functional> (required)... yes\n",
            "checking if g++ supports stl <algorithm> (required)... yes\n",
            "checking if g++ supports stl <string> (required)... yes\n",
            "checking if g++ supports stl <iostream> (required)... yes\n",
            "checking if g++ supports stl <sstream> (required)... yes\n",
            "checking if g++ supports stl <fstream> (required)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports const_cast<> (required)... yes\n",
            "checking if g++ supports static_cast<> (required)... yes\n",
            "checking if g++ supports reinterpret_cast<> (required)... yes\n",
            "checking if g++ supports namespaces (required) ... yes\n",
            "checking if g++ supports __thread (optional)... yes\n",
            "checking if g++ supports template <class T> (required)... yes\n",
            "checking if g++ supports GCC native atomic operations (optional)... yes\n",
            "checking if g++ supports OSX native atomic operations (optional)... no\n",
            "checking if g++ environment provides all required features... yes\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating src/Makefile\n",
            "config.status: creating src/Makefile.msvc\n",
            "config.status: creating man/Makefile\n",
            "config.status: creating doc/Makefile\n",
            "config.status: creating tests/Makefile\n",
            "config.status: creating swig/version.h\n",
            "config.status: creating mecab.iss\n",
            "config.status: creating mecab-config\n",
            "config.status: creating mecabrc\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "config.status: executing default commands\n",
            "make  all-recursive\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "Making all in src\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o viterbi.lo viterbi.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c viterbi.cpp  -fPIC -DPIC -o .libs/viterbi.o\n",
            "In file included from \u001b[01m\u001b[Kviterbi.cpp:14:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kparam.h:30:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[KTarget {anonymous}::lexical_cast(Source) [with Target = std::__cxx11::basic_string<char>; Source = std::__cxx11::basic_string<char>]\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " std::string \u001b[01;35m\u001b[Klexical_cast<std::string, std::string>\u001b[m\u001b[K(std::string arg) {\n",
            "             \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c viterbi.cpp -o viterbi.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o tagger.lo tagger.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tagger.cpp  -fPIC -DPIC -o .libs/tagger.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tagger.cpp -o tagger.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o utils.lo utils.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c utils.cpp  -fPIC -DPIC -o .libs/utils.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c utils.cpp -o utils.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o eval.lo eval.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c eval.cpp  -fPIC -DPIC -o .libs/eval.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c eval.cpp -o eval.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o iconv_utils.lo iconv_utils.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c iconv_utils.cpp  -fPIC -DPIC -o .libs/iconv_utils.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c iconv_utils.cpp -o iconv_utils.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_rewriter.lo dictionary_rewriter.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_rewriter.cpp  -fPIC -DPIC -o .libs/dictionary_rewriter.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_rewriter.cpp -o dictionary_rewriter.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_generator.lo dictionary_generator.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_generator.cpp  -fPIC -DPIC -o .libs/dictionary_generator.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_generator.cpp -o dictionary_generator.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary_compiler.lo dictionary_compiler.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_compiler.cpp  -fPIC -DPIC -o .libs/dictionary_compiler.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary_compiler.cpp -o dictionary_compiler.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o context_id.lo context_id.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c context_id.cpp  -fPIC -DPIC -o .libs/context_id.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c context_id.cpp -o context_id.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o connector.lo connector.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c connector.cpp  -fPIC -DPIC -o .libs/connector.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c connector.cpp -o connector.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o nbest_generator.lo nbest_generator.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c nbest_generator.cpp  -fPIC -DPIC -o .libs/nbest_generator.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c nbest_generator.cpp -o nbest_generator.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o writer.lo writer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c writer.cpp  -fPIC -DPIC -o .libs/writer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c writer.cpp -o writer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o string_buffer.lo string_buffer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c string_buffer.cpp  -fPIC -DPIC -o .libs/string_buffer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c string_buffer.cpp -o string_buffer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o param.lo param.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c param.cpp  -fPIC -DPIC -o .libs/param.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c param.cpp -o param.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o tokenizer.lo tokenizer.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tokenizer.cpp  -fPIC -DPIC -o .libs/tokenizer.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c tokenizer.cpp -o tokenizer.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o char_property.lo char_property.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c char_property.cpp  -fPIC -DPIC -o .libs/char_property.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c char_property.cpp -o char_property.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o dictionary.lo dictionary.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary.cpp  -fPIC -DPIC -o .libs/dictionary.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c dictionary.cpp -o dictionary.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o feature_index.lo feature_index.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c feature_index.cpp  -fPIC -DPIC -o .libs/feature_index.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c feature_index.cpp -o feature_index.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o lbfgs.lo lbfgs.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c lbfgs.cpp  -fPIC -DPIC -o .libs/lbfgs.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c lbfgs.cpp -o lbfgs.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o learner_tagger.lo learner_tagger.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner_tagger.cpp  -fPIC -DPIC -o .libs/learner_tagger.o\n",
            "\u001b[01m\u001b[Klearner_tagger.cpp:25:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K'\u001b[01m\u001b[Kchar* MeCab::{anonymous}::mystrdup(const string&)\u001b[m\u001b[K' defined but not used [\u001b[01;35m\u001b[K-Wunused-function\u001b[m\u001b[K]\n",
            " char *\u001b[01;35m\u001b[Kmystrdup\u001b[m\u001b[K(const std::string &str) {\n",
            "       \u001b[01;35m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner_tagger.cpp -o learner_tagger.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o learner.lo learner.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner.cpp  -fPIC -DPIC -o .libs/learner.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c learner.cpp -o learner.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=compile g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o libmecab.lo libmecab.cpp\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c libmecab.cpp  -fPIC -DPIC -o .libs/libmecab.o\n",
            "libtool: compile:  g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102 -DMECAB_DEFAULT_RC=\\\"/usr/local/etc/mecabrc\\\" -O3 -Wall -c libmecab.cpp -o libmecab.o >/dev/null 2>&1\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall  -no-undefined -version-info 2:0:0  -o libmecab.la -rpath /usr/local/lib viterbi.lo tagger.lo utils.lo eval.lo iconv_utils.lo dictionary_rewriter.lo dictionary_generator.lo dictionary_compiler.lo context_id.lo connector.lo nbest_generator.lo writer.lo string_buffer.lo param.lo tokenizer.lo char_property.lo dictionary.lo feature_index.lo lbfgs.lo learner_tagger.lo learner.lo libmecab.lo  -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++  -fPIC -DPIC -shared -nostdlib /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/7/crtbeginS.o  .libs/viterbi.o .libs/tagger.o .libs/utils.o .libs/eval.o .libs/iconv_utils.o .libs/dictionary_rewriter.o .libs/dictionary_generator.o .libs/dictionary_compiler.o .libs/context_id.o .libs/connector.o .libs/nbest_generator.o .libs/writer.o .libs/string_buffer.o .libs/param.o .libs/tokenizer.o .libs/char_property.o .libs/dictionary.o .libs/feature_index.o .libs/lbfgs.o .libs/learner_tagger.o .libs/learner.o .libs/libmecab.o   -lpthread -L/usr/lib/gcc/x86_64-linux-gnu/7 -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/7/../../../../lib -L/lib/x86_64-linux-gnu -L/lib/../lib -L/usr/lib/x86_64-linux-gnu -L/usr/lib/../lib -L/usr/local/cuda/lib64/stubs -L/usr/lib/gcc/x86_64-linux-gnu/7/../../.. -lstdc++ -lm -lc -lgcc_s /usr/lib/gcc/x86_64-linux-gnu/7/crtendS.o /usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/crtn.o  -O3   -Wl,-soname -Wl,libmecab.so.2 -o .libs/libmecab.so.2.0.0\n",
            "libtool: link: (cd \".libs\" && rm -f \"libmecab.so.2\" && ln -s \"libmecab.so.2.0.0\" \"libmecab.so.2\")\n",
            "libtool: link: (cd \".libs\" && rm -f \"libmecab.so\" && ln -s \"libmecab.so.2.0.0\" \"libmecab.so\")\n",
            "libtool: link: ar cru .libs/libmecab.a  viterbi.o tagger.o utils.o eval.o iconv_utils.o dictionary_rewriter.o dictionary_generator.o dictionary_compiler.o context_id.o connector.o nbest_generator.o writer.o string_buffer.o param.o tokenizer.o char_property.o dictionary.o feature_index.o lbfgs.o learner_tagger.o learner.o libmecab.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "libtool: link: ranlib .libs/libmecab.a\n",
            "libtool: link: ( cd \".libs\" && rm -f \"libmecab.la\" && ln -s \"../libmecab.la\" \"libmecab.la\" )\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab.o mecab.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab mecab.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab mecab.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-dict-index.o mecab-dict-index.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-dict-index mecab-dict-index.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-dict-index mecab-dict-index.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-dict-gen.o mecab-dict-gen.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-dict-gen mecab-dict-gen.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-dict-gen mecab-dict-gen.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-cost-train.o mecab-cost-train.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-cost-train mecab-cost-train.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-cost-train mecab-cost-train.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-system-eval.o mecab-system-eval.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-system-eval mecab-system-eval.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-system-eval mecab-system-eval.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "g++ -DHAVE_CONFIG_H -I. -I.. -DDIC_VERSION=102   -DMECAB_DEFAULT_RC=\"\\\"/usr/local/etc/mecabrc\\\"\"    -O3 -Wall  -c -o mecab-test-gen.o mecab-test-gen.cpp\n",
            "/bin/bash ../libtool --tag=CXX   --mode=link g++  -O3 -Wall    -o mecab-test-gen mecab-test-gen.o libmecab.la -lpthread -lpthread  -lstdc++ \n",
            "libtool: link: g++ -O3 -Wall -o .libs/mecab-test-gen mecab-test-gen.o  ./.libs/libmecab.so -lpthread -lstdc++\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "Making all in man\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "Making all in doc\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "Making all in tests\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Nothing to be done for 'all'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "Making check in src\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "Making check in man\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "Making check in doc\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[1]: Nothing to be done for 'check'.\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "Making check in tests\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make  check-TESTS\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 177\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 178x178\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 83\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 84x84\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 450\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 162\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 3x3\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 4\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 11\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 1\n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./unk.def ... 2\n",
            "emitting double-array: 100% |###########################################| \n",
            "./model.def is not found. skipped.\n",
            "./pos-id.def is not found. minimum setting is used\n",
            "reading ./dic.csv ... 1\n",
            "reading ./matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "PASS: run-dics.sh\n",
            "PASS: run-eval.sh\n",
            "seed/pos-id.def is not found. minimum setting is used\n",
            "reading seed/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "seed/model.def is not found. skipped.\n",
            "seed/pos-id.def is not found. minimum setting is used\n",
            "reading seed/dic.csv ... 4335\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading seed/matrix.def ... 1x1\n",
            "\n",
            "done!\n",
            "reading corpus ...\n",
            "Number of sentences: 34\n",
            "Number of features:  64108\n",
            "eta:                 0.00005\n",
            "freq:                1\n",
            "eval-size:           6\n",
            "unk-eval-size:       4\n",
            "threads:             1\n",
            "charset:             EUC-JP\n",
            "C(sigma^2):          1.00000\n",
            "\n",
            "iter=0 err=1.00000 F=0.35771 target=2406.28355 diff=1.00000\n",
            "iter=1 err=0.97059 F=0.65652 target=1484.25231 diff=0.38318\n",
            "iter=2 err=0.91176 F=0.79331 target=863.32765 diff=0.41834\n",
            "iter=3 err=0.85294 F=0.89213 target=596.72480 diff=0.30881\n",
            "iter=4 err=0.61765 F=0.95467 target=336.30744 diff=0.43641\n",
            "iter=5 err=0.50000 F=0.96702 target=246.53039 diff=0.26695\n",
            "iter=6 err=0.35294 F=0.95472 target=188.93963 diff=0.23361\n",
            "iter=7 err=0.20588 F=0.99106 target=168.62665 diff=0.10751\n",
            "iter=8 err=0.05882 F=0.99777 target=158.64865 diff=0.05917\n",
            "iter=9 err=0.08824 F=0.99665 target=154.14530 diff=0.02839\n",
            "iter=10 err=0.08824 F=0.99665 target=151.94257 diff=0.01429\n",
            "iter=11 err=0.02941 F=0.99888 target=147.20825 diff=0.03116\n",
            "iter=12 err=0.00000 F=1.00000 target=147.34956 diff=0.00096\n",
            "iter=13 err=0.02941 F=0.99888 target=146.32592 diff=0.00695\n",
            "iter=14 err=0.00000 F=1.00000 target=145.77299 diff=0.00378\n",
            "iter=15 err=0.02941 F=0.99888 target=145.24641 diff=0.00361\n",
            "iter=16 err=0.00000 F=1.00000 target=144.96490 diff=0.00194\n",
            "iter=17 err=0.02941 F=0.99888 target=144.90246 diff=0.00043\n",
            "iter=18 err=0.00000 F=1.00000 target=144.75959 diff=0.00099\n",
            "iter=19 err=0.00000 F=1.00000 target=144.71727 diff=0.00029\n",
            "iter=20 err=0.00000 F=1.00000 target=144.66337 diff=0.00037\n",
            "iter=21 err=0.00000 F=1.00000 target=144.61349 diff=0.00034\n",
            "iter=22 err=0.00000 F=1.00000 target=144.62987 diff=0.00011\n",
            "iter=23 err=0.00000 F=1.00000 target=144.60060 diff=0.00020\n",
            "iter=24 err=0.00000 F=1.00000 target=144.59125 diff=0.00006\n",
            "iter=25 err=0.00000 F=1.00000 target=144.58619 diff=0.00004\n",
            "iter=26 err=0.00000 F=1.00000 target=144.58219 diff=0.00003\n",
            "iter=27 err=0.00000 F=1.00000 target=144.58059 diff=0.00001\n",
            "\n",
            "Done! writing model file ... \n",
            "model-ipadic.c1.0.f1.model is not a binary model. reopen it as text mode...\n",
            "reading seed/unk.def ... 40\n",
            "reading seed/dic.csv ... 4335\n",
            "emitting model-ipadic.c1.0.f1.dic/left-id.def/ model-ipadic.c1.0.f1.dic/right-id.def\n",
            "emitting model-ipadic.c1.0.f1.dic/unk.def ... 40\n",
            "emitting model-ipadic.c1.0.f1.dic/dic.csv ... 4335\n",
            "emitting matrix      : 100% |###########################################| \n",
            "copying seed/char.def to model-ipadic.c1.0.f1.dic/char.def\n",
            "copying seed/rewrite.def to model-ipadic.c1.0.f1.dic/rewrite.def\n",
            "copying seed/dicrc to model-ipadic.c1.0.f1.dic/dicrc\n",
            "copying seed/feature.def to model-ipadic.c1.0.f1.dic/feature.def\n",
            "copying model-ipadic.c1.0.f1.model to model-ipadic.c1.0.f1.dic/model.def\n",
            "\n",
            "done!\n",
            "model-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\n",
            "reading model-ipadic.c1.0.f1.dic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "model-ipadic.c1.0.f1.dic/pos-id.def is not found. minimum setting is used\n",
            "reading model-ipadic.c1.0.f1.dic/dic.csv ... 4335\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading model-ipadic.c1.0.f1.dic/matrix.def ... 346x346\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "              precision          recall         F\n",
            "LEVEL 0:    12.8959(57/442) 11.8998(57/479) 12.3779\n",
            "LEVEL 1:    12.2172(54/442) 11.2735(54/479) 11.7264\n",
            "LEVEL 2:    11.7647(52/442) 10.8559(52/479) 11.2921\n",
            "LEVEL 4:    11.7647(52/442) 10.8559(52/479) 11.2921\n",
            "PASS: run-cost-train.sh\n",
            "==================\n",
            "All 3 tests passed\n",
            "==================\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "Making install in src\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "test -z \"/usr/local/lib\" || /bin/mkdir -p \"/usr/local/lib\"\n",
            " /bin/bash ../libtool   --mode=install /usr/bin/install -c   libmecab.la '/usr/local/lib'\n",
            "libtool: install: /usr/bin/install -c .libs/libmecab.so.2.0.0 /usr/local/lib/libmecab.so.2.0.0\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so.2 || { rm -f libmecab.so.2 && ln -s libmecab.so.2.0.0 libmecab.so.2; }; })\n",
            "libtool: install: (cd /usr/local/lib && { ln -s -f libmecab.so.2.0.0 libmecab.so || { rm -f libmecab.so && ln -s libmecab.so.2.0.0 libmecab.so; }; })\n",
            "libtool: install: /usr/bin/install -c .libs/libmecab.lai /usr/local/lib/libmecab.la\n",
            "libtool: install: /usr/bin/install -c .libs/libmecab.a /usr/local/lib/libmecab.a\n",
            "libtool: install: chmod 644 /usr/local/lib/libmecab.a\n",
            "libtool: install: ranlib /usr/local/lib/libmecab.a\n",
            "libtool: finish: PATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/sbin\" ldconfig -n /usr/local/lib\n",
            "----------------------------------------------------------------------\n",
            "Libraries have been installed in:\n",
            "   /usr/local/lib\n",
            "\n",
            "If you ever happen to want to link against installed libraries\n",
            "in a given directory, LIBDIR, you must either use libtool, and\n",
            "specify the full pathname of the library, or use the `-LLIBDIR'\n",
            "flag during linking and do at least one of the following:\n",
            "   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable\n",
            "     during execution\n",
            "   - add LIBDIR to the `LD_RUN_PATH' environment variable\n",
            "     during linking\n",
            "   - use the `-Wl,-rpath -Wl,LIBDIR' linker flag\n",
            "   - have your system administrator add LIBDIR to `/etc/ld.so.conf'\n",
            "\n",
            "See any operating system documentation about shared libraries for\n",
            "more information, such as the ld(1) and ld.so(8) manual pages.\n",
            "----------------------------------------------------------------------\n",
            "test -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab '/usr/local/bin'\n",
            "libtool: install: /usr/bin/install -c .libs/mecab /usr/local/bin/mecab\n",
            "test -z \"/usr/local/libexec/mecab\" || /bin/mkdir -p \"/usr/local/libexec/mecab\"\n",
            "  /bin/bash ../libtool   --mode=install /usr/bin/install -c mecab-dict-index mecab-dict-gen mecab-cost-train mecab-system-eval mecab-test-gen '/usr/local/libexec/mecab'\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-dict-index /usr/local/libexec/mecab/mecab-dict-index\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-dict-gen /usr/local/libexec/mecab/mecab-dict-gen\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-cost-train /usr/local/libexec/mecab/mecab-cost-train\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-system-eval /usr/local/libexec/mecab/mecab-system-eval\n",
            "libtool: install: /usr/bin/install -c .libs/mecab-test-gen /usr/local/libexec/mecab/mecab-test-gen\n",
            "test -z \"/usr/local/include\" || /bin/mkdir -p \"/usr/local/include\"\n",
            " /usr/bin/install -c -m 644 mecab.h '/usr/local/include'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/src'\n",
            "Making install in man\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            "test -z \"/usr/local/share/man/man1\" || /bin/mkdir -p \"/usr/local/share/man/man1\"\n",
            " /usr/bin/install -c -m 644 mecab.1 '/usr/local/share/man/man1'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/man'\n",
            "Making install in doc\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            "make[2]: Nothing to be done for 'install-data-am'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/doc'\n",
            "Making install in tests\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[2]: Nothing to be done for 'install-exec-am'.\n",
            "make[2]: Nothing to be done for 'install-data-am'.\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2/tests'\n",
            "make[1]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[2]: Entering directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "test -z \"/usr/local/bin\" || /bin/mkdir -p \"/usr/local/bin\"\n",
            " /usr/bin/install -c mecab-config '/usr/local/bin'\n",
            "test -z \"/usr/local/etc\" || /bin/mkdir -p \"/usr/local/etc\"\n",
            " /usr/bin/install -c -m 644 mecabrc '/usr/local/etc'\n",
            "make[2]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "make[1]: Leaving directory '/tmp/mecab-0.996-ko-0.9.2'\n",
            "Install mecab-ko-dic\n",
            "Install mecab-ko-dic\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 47.4M  100 47.4M    0     0  21.3M      0  0:00:02  0:00:02 --:--:-- 26.5M\n",
            "mecab-ko-dic-2.1.1-20180720/\n",
            "mecab-ko-dic-2.1.1-20180720/configure\n",
            "mecab-ko-dic-2.1.1-20180720/COPYING\n",
            "mecab-ko-dic-2.1.1-20180720/autogen.sh\n",
            "mecab-ko-dic-2.1.1-20180720/Place-station.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NNG.csv\n",
            "mecab-ko-dic-2.1.1-20180720/README\n",
            "mecab-ko-dic-2.1.1-20180720/EF.csv\n",
            "mecab-ko-dic-2.1.1-20180720/MAG.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Preanalysis.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NNB.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Person-actor.csv\n",
            "mecab-ko-dic-2.1.1-20180720/VV.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Makefile.in\n",
            "mecab-ko-dic-2.1.1-20180720/matrix.def\n",
            "mecab-ko-dic-2.1.1-20180720/EC.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NNBC.csv\n",
            "mecab-ko-dic-2.1.1-20180720/clean\n",
            "mecab-ko-dic-2.1.1-20180720/ChangeLog\n",
            "mecab-ko-dic-2.1.1-20180720/J.csv\n",
            "mecab-ko-dic-2.1.1-20180720/.keep\n",
            "mecab-ko-dic-2.1.1-20180720/feature.def\n",
            "mecab-ko-dic-2.1.1-20180720/Foreign.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XPN.csv\n",
            "mecab-ko-dic-2.1.1-20180720/EP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/NR.csv\n",
            "mecab-ko-dic-2.1.1-20180720/left-id.def\n",
            "mecab-ko-dic-2.1.1-20180720/Place.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Symbol.csv\n",
            "mecab-ko-dic-2.1.1-20180720/dicrc\n",
            "mecab-ko-dic-2.1.1-20180720/NP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/ETM.csv\n",
            "mecab-ko-dic-2.1.1-20180720/IC.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Place-address.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Group.csv\n",
            "mecab-ko-dic-2.1.1-20180720/model.def\n",
            "mecab-ko-dic-2.1.1-20180720/XSN.csv\n",
            "mecab-ko-dic-2.1.1-20180720/INSTALL\n",
            "mecab-ko-dic-2.1.1-20180720/rewrite.def\n",
            "mecab-ko-dic-2.1.1-20180720/Inflect.csv\n",
            "mecab-ko-dic-2.1.1-20180720/configure.ac\n",
            "mecab-ko-dic-2.1.1-20180720/NNP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/CoinedWord.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XSV.csv\n",
            "mecab-ko-dic-2.1.1-20180720/pos-id.def\n",
            "mecab-ko-dic-2.1.1-20180720/Makefile.am\n",
            "mecab-ko-dic-2.1.1-20180720/unk.def\n",
            "mecab-ko-dic-2.1.1-20180720/missing\n",
            "mecab-ko-dic-2.1.1-20180720/VCP.csv\n",
            "mecab-ko-dic-2.1.1-20180720/install-sh\n",
            "mecab-ko-dic-2.1.1-20180720/Hanja.csv\n",
            "mecab-ko-dic-2.1.1-20180720/MAJ.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XSA.csv\n",
            "mecab-ko-dic-2.1.1-20180720/Wikipedia.csv\n",
            "mecab-ko-dic-2.1.1-20180720/tools/\n",
            "mecab-ko-dic-2.1.1-20180720/tools/add-userdic.sh\n",
            "mecab-ko-dic-2.1.1-20180720/tools/mecab-bestn.sh\n",
            "mecab-ko-dic-2.1.1-20180720/tools/convert_for_using_store.sh\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/place.csv\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/person.csv\n",
            "mecab-ko-dic-2.1.1-20180720/user-dic/README.md\n",
            "mecab-ko-dic-2.1.1-20180720/NorthKorea.csv\n",
            "mecab-ko-dic-2.1.1-20180720/VX.csv\n",
            "mecab-ko-dic-2.1.1-20180720/right-id.def\n",
            "mecab-ko-dic-2.1.1-20180720/VA.csv\n",
            "mecab-ko-dic-2.1.1-20180720/char.def\n",
            "mecab-ko-dic-2.1.1-20180720/NEWS\n",
            "mecab-ko-dic-2.1.1-20180720/MM.csv\n",
            "mecab-ko-dic-2.1.1-20180720/ETN.csv\n",
            "mecab-ko-dic-2.1.1-20180720/AUTHORS\n",
            "mecab-ko-dic-2.1.1-20180720/Person.csv\n",
            "mecab-ko-dic-2.1.1-20180720/XR.csv\n",
            "mecab-ko-dic-2.1.1-20180720/VCN.csv\n",
            "Looking in current directory for macros.\n",
            "configure.ac:2: warning: AM_INIT_AUTOMAKE: two- and three-arguments forms are deprecated.  For more info, see:\n",
            "configure.ac:2: http://www.gnu.org/software/automake/manual/automake.html#Modernize-AM_005fINIT_005fAUTOMAKE-invocation\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "/tmp/mecab-ko-dic-2.1.1-20180720/missing: Unknown `--is-lightweight' option\n",
            "Try `/tmp/mecab-ko-dic-2.1.1-20180720/missing --help' for more information\n",
            "configure: WARNING: 'missing' script is too old or missing\n",
            "checking for a thread-safe mkdir -p... /bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports nested variables... yes\n",
            "checking for mecab-config... /usr/local/bin/mecab-config\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "/usr/local/lib\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "/usr/local/libexec/mecab/mecab-dict-index -d . -o . -f UTF-8 -t UTF-8\n",
            "reading ./unk.def ... 13\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./NR.csv ... 482\n",
            "reading ./IC.csv ... 1305\n",
            "reading ./CoinedWord.csv ... 148\n",
            "reading ./Place.csv ... 30303\n",
            "reading ./VX.csv ... 125\n",
            "reading ./EP.csv ... 51\n",
            "reading ./NNBC.csv ... 677\n",
            "reading ./NorthKorea.csv ... 3\n",
            "reading ./Wikipedia.csv ... 36762\n",
            "reading ./VCP.csv ... 9\n",
            "reading ./MAG.csv ... 14242\n",
            "reading ./NNP.csv ... 2371\n",
            "reading ./VCN.csv ... 7\n",
            "reading ./Inflect.csv ... 44820\n",
            "reading ./XPN.csv ... 83\n",
            "reading ./MM.csv ... 453\n",
            "reading ./ETN.csv ... 14\n",
            "reading ./XSA.csv ... 19\n",
            "reading ./Symbol.csv ... 16\n",
            "reading ./Place-address.csv ... 19301\n",
            "reading ./Place-station.csv ... 1145\n",
            "reading ./XSV.csv ... 23\n",
            "reading ./MAJ.csv ... 240\n",
            "reading ./Preanalysis.csv ... 5\n",
            "reading ./ETM.csv ... 133\n",
            "reading ./NNB.csv ... 140\n",
            "reading ./Group.csv ... 3176\n",
            "reading ./Hanja.csv ... 125750\n",
            "reading ./J.csv ... 416\n",
            "reading ./EC.csv ... 2547\n",
            "reading ./Person.csv ... 196459\n",
            "reading ./XSN.csv ... 124\n",
            "reading ./EF.csv ... 1820\n",
            "reading ./VV.csv ... 7331\n",
            "reading ./Foreign.csv ... 11690\n",
            "reading ./Person-actor.csv ... 99230\n",
            "reading ./NP.csv ... 342\n",
            "reading ./VA.csv ... 2360\n",
            "reading ./XR.csv ... 3637\n",
            "reading ./NNG.csv ... 208524\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading ./matrix.def ... 3822x2693\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "echo To enable dictionary, rewrite /usr/local/etc/mecabrc as \\\"dicdir = /usr/local/lib/mecab/dic/mecab-ko-dic\\\"\n",
            "To enable dictionary, rewrite /usr/local/etc/mecabrc as \"dicdir = /usr/local/lib/mecab/dic/mecab-ko-dic\"\n",
            "make[1]: Entering directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
            "make[1]: Nothing to be done for 'install-exec-am'.\n",
            " /bin/mkdir -p '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
            " /usr/bin/install -c -m 644 model.bin matrix.bin char.bin sys.dic unk.dic left-id.def right-id.def rewrite.def pos-id.def dicrc '/usr/local/lib/mecab/dic/mecab-ko-dic'\n",
            "make[1]: Leaving directory '/tmp/mecab-ko-dic-2.1.1-20180720'\n",
            "Install mecab-python\n",
            "/tmp /tmp/mecab-ko-dic-2.1.1-20180720\n",
            "Cloning into 'mecab-python-0.996'...\n",
            "remote: Counting objects: 17, done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 17 (delta 3), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "/tmp/mecab-ko-dic-2.1.1-20180720\n",
            "Processing /tmp/mecab-python-0.996\n",
            "Building wheels for collected packages: mecab-python\n",
            "  Building wheel for mecab-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python: filename=mecab_python-0.996_ko_0.9.2-cp36-cp36m-linux_x86_64.whl size=140569 sha256=0c6695efb2e85084a82cb4a4a86344753357e5dc3d3ac8090ca421ad77bcad67\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/75/a6/e9e73a1dbd73579383644942ef18a6d17ad728a3052a1147fb\n",
            "Successfully built mecab-python\n",
            "Installing collected packages: mecab-python\n",
            "Successfully installed mecab-python-0.996-ko-0.9.2\n",
            "Done.\n",
            "mecab 설치 소요 시간: 1.82 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w58sp-QgRZay",
        "outputId": "b7ad2762-face-4007-d883-897ea74eca78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "!python -m spacy download en\n",
        "\n",
        "print(f'spacy tokenizer(en) 설치 소요 시간: {((time.time()-start)/60):.2f} min')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "spacy tokenizer(en) 설치 소요 시간: 0.08 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHLng0G3RZdJ",
        "outputId": "4c4b5c94-b394-48ff-ee49-918488157763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "from konlpy.tag import Mecab\n",
        "import spacy\n",
        "\n",
        "mecab = Mecab()\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "print('tokenization test with sample texts')\n",
        "print('tokenizing for Korean with mecab: ', mecab.morphs('안녕하세요 저는 건국대학교에 재학 중인 박도준입니다.'))\n",
        "print('tokenizing for English: ', tokenize_en('Hello, I am Dojun Park, a student at Konkuk University.'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenization test with sample texts\n",
            "tokenizing for Korean with mecab:  ['안녕', '하', '세요', '저', '는', '건국대', '학교', '에', '재학', '중', '인', '박도준', '입니다', '.']\n",
            "tokenizing for English:  ['Hello', ',', 'I', 'am', 'Dojun', 'Park', ',', 'a', 'student', 'at', 'Konkuk', 'University', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHEjDCUrVdFc"
      },
      "source": [
        "# preparing the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH9DiKZNRZf1",
        "outputId": "5842e685-4f71-48f3-a27a-67bf1bf8b89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "!pip uninstall torchtext -y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling torchtext-0.3.1:\n",
            "  Successfully uninstalled torchtext-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdtiTA_XRZin",
        "outputId": "66f14c14-17de-401a-d445-b83a503087d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/f9/224b3893ab11d83d47fde357a7dcc75f00ba219f34f3d15e06fe4cb62e05/torchtext-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (4.5MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5MB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "Successfully installed sentencepiece-0.1.91 torchtext-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENNzlYBeRZYW",
        "outputId": "4031d237-1f32-4e43-c963-d86d4666a31d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "source": [
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "\n",
        "korean = Field(tokenize=mecab.morphs, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "english = Field(tokenize=tokenize_en, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "\n",
        "fields = {'kor': ('src', korean), 'eng':('trg', english)}\n",
        "\n",
        "train_data, valid_data, test_data = TabularDataset.splits(\n",
        "                                                    path = '/content/drive/My Drive/Colab Notebooks',\n",
        "                                                    train = 'train2.csv',\n",
        "                                                    validation = 'valid2.csv',\n",
        "                                                    test = 'test2.csv',\n",
        "                                                    format = 'csv',\n",
        "                                                    fields = fields)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnQQNBqgWav7"
      },
      "source": [
        "korean.build_vocab(train_data, min_freq=2)\n",
        "english.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64VRgFq1VOHc",
        "outputId": "59323c35-3bca-41b4-c664-4fcb3d370b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(train_data[0].__dict__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['영화', '가', '관객', '의', '압도', '적', '인', '지지', '를', '받', '으며', '덩달', '아', '퀸', '신드롬', '까지', '일으켰', '다', '.'], 'trg': ['the', 'movie', 'even', 'created', 'queen', 'syndrome', 'with', 'overwhelming', 'support', 'from', 'the', 'audience', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cknln9SRVOMX",
        "outputId": "723f97ab-2bfd-4e21-b316-86949f05ebfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(valid_data[0].__dict__)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['중국', '이', '미국', '산', '대두', ',', '보잉', '사', ',', '자동차', ',', '반도체', ',', '휴대폰', '을', '언급', '한', '이유', '다', '.'], 'trg': ['that', \"'s\", 'why', 'china', 'mentioned', 'u.s.', 'soybeans', ',', 'boeing', ',', 'automobiles', ',', 'semiconductors', 'and', 'cell', 'phones', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1t1xaIkVORB",
        "outputId": "519090ce-a2c9-4b16-de92-3e5a0094a0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(test_data[0].__dict__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['중', '장기', '적', '으로', 'cj', 'enm', '의', '기업', '가치', '가', '상승', '할', '것', '이', '란', '전망', '을', '내놓', '고', '있', '다', '.'], 'trg': ['it', 'is', 'predicting', 'that', 'cj', 'enm', \"'s\", 'corporate', 'value', 'will', 'rise', 'in', 'the', 'mid-', 'to', 'long', '-', 'term', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB4BOprjWNnB"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWZoyf5fVOVS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        src_pad_idx,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        max_length,\n",
        "        device):\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.device = device\n",
        "        self.transformer = nn.Transformer(\n",
        "            embedding_size,\n",
        "            num_heads,\n",
        "            num_encoder_layers,\n",
        "            num_decoder_layers,\n",
        "            forward_expansion,\n",
        "            dropout)\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # src shape: (src_len, N)\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
        "        # (N, src_len)\n",
        "        return src_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_seq_length, N = src.shape\n",
        "        trg_seq_length, N = trg.shape\n",
        "\n",
        "        src_positions = (\n",
        "            torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        trg_positions = (\n",
        "            torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        embed_src = self.dropout(\n",
        "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
        "        )\n",
        "\n",
        "        embed_trg = self.dropout(\n",
        "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
        "        )\n",
        "\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
        "            self.device)\n",
        "\n",
        "        out = self.transformer(\n",
        "            embed_src,\n",
        "            embed_trg,\n",
        "            src_key_padding_mask = src_padding_mask,\n",
        "            tgt_mask = trg_mask\n",
        "        )\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuEdHiDbVOYb",
        "outputId": "e435dca2-d060-4999-8b51-eb7c0c324f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Setup the training phase\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "load_model = False\n",
        "save_model = True\n",
        "\n",
        "# Training hyperparameters\n",
        "num_epochs = 5\n",
        "learning_rate = 1e-4\n",
        "batch_size = 32\n",
        "\n",
        "# Model hyperparameters\n",
        "src_vocab_size = len(korean.vocab)\n",
        "trg_vocab_size = len(english.vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 6  # in the paper 6\n",
        "num_decoder_layers = 6\n",
        "dropout = 0.10\n",
        "max_len = 100\n",
        "forward_expansion = 4\n",
        "src_pad_idx = english.vocab.stoi['<pad>']\n",
        "\n",
        "# Tensorboard for nice plots\n",
        "writer = SummaryWriter('runs/loss_plot')\n",
        "step = 0\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = batch_size,\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x: len(x.src),\n",
        "    device = device\n",
        ")\n",
        "\n",
        "model = Transformer(\n",
        "    embedding_size,\n",
        "    src_vocab_size,\n",
        "    trg_vocab_size,\n",
        "    src_pad_idx,\n",
        "    num_heads,\n",
        "    num_encoder_layers,\n",
        "    num_decoder_layers,\n",
        "    forward_expansion,\n",
        "    dropout,\n",
        "    max_len,\n",
        "    device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "pad_idx = english.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1Yg2myrXQIG"
      },
      "source": [
        "def translate_sentence(model, sentence, korean, english, device, max_length=50):\n",
        "    \n",
        "    tokens = mecab.morphs(sentence)\n",
        "    # tokens = komoran.morphs(sentence)\n",
        "    # tokens = tokenize_khaiii(sentence)\n",
        "\n",
        "    # Add <SOS> and <EOS> in beginning and end respectively\n",
        "    tokens.insert(0, korean.init_token)\n",
        "    tokens.append(korean.eos_token)\n",
        "\n",
        "    # Go through each korean token and convert to an index\n",
        "    text_to_indices = [korean.vocab.stoi[token] for token in tokens]\n",
        "\n",
        "    # Convert to Tensor\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "    for i in range(max_length):\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
        "    # remove start token\n",
        "    return translated_sentence[1:]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms1JY1hiWlaV"
      },
      "source": [
        "# train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AhMM-QgVOPS",
        "outputId": "acdda9ed-b908-4aff-8bb5-45a6ef586365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.eval()\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_iterator):\n",
        "        inp_data = batch.src.to(device)\n",
        "        target = batch.trg.to(device)\n",
        "\n",
        "        # forward prop\n",
        "        output = model(inp_data, target[:-1])\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        writer.add_scalar('Training loss', loss, global_step=step)\n",
        "        step += 1\n",
        "\n",
        "\n",
        "    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\n",
        "    translated_sentence = ' '.join(translated_sentence)\n",
        "    translated_sentence = translated_sentence.replace(' ,', ',')\n",
        "    translated_sentence = translated_sentence.replace(' .', '.')\n",
        "    translated_sentence = translated_sentence.replace(' <eos>', '')\n",
        "\n",
        "    print(f'[Epoch] {epoch+1} / {num_epochs}')\n",
        "    print(f'[Loss] {loss:.4f}')\n",
        "    print(f'[Exsample] {sentence} >>> {translated_sentence}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch] 1 / 5\n",
            "[Loss] 5.5955\n",
            "[Exsample] 이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다. >>> the government is the <unk> is the government of the <unk> is the <unk>.\n",
            "[Epoch] 2 / 5\n",
            "[Loss] 5.7968\n",
            "[Exsample] 이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다. >>> this is because the most are the world of the <unk> of the year.\n",
            "[Epoch] 3 / 5\n",
            "[Loss] 5.0973\n",
            "[Exsample] 이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다. >>> many people are many many many people in the <unk> of the <unk>.\n",
            "[Epoch] 4 / 5\n",
            "[Loss] 4.6091\n",
            "[Exsample] 이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다. >>> there are many many people in the people of the <unk> of the <unk>.\n",
            "[Epoch] 5 / 5\n",
            "[Loss] 4.9534\n",
            "[Exsample] 이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다. >>> many people are many of the country of the <unk> are many.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHtJ9pwxW-ux"
      },
      "source": [
        "# test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afvh8VEEVOKO",
        "outputId": "dd0c9342-f45b-44d3-f449-d8f571620f4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def bleu(data, model, korean, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, korean, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "bleu(test_data, model, korean, english, device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a09f237f7bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkorean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-a09f237f7bd0>\u001b[0m in \u001b[0;36mbleu\u001b[0;34m(data, model, korean, english, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkorean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# remove <eos> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-410fce9e17fb>\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(model, sentence, korean, english, device, max_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkorean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmecab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# tokens = komoran.morphs(sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# tokens = tokenize_khaiii(sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36mmorphs\u001b[0;34m(self, phrase)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;34m\"\"\"Parse phrase to morphemes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, flatten, join)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/MeCab.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0m__getattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Wrong number or type of arguments for overloaded function 'Tagger_parse'.\n  Possible C/C++ prototypes are:\n    MeCab::Tagger::parse(MeCab::Model const &,MeCab::Lattice *)\n    MeCab::Tagger::parse(MeCab::Lattice *) const\n    MeCab::Tagger::parse(char const *)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbOYjaDGa2OA"
      },
      "source": [
        "# look into the data structure\n",
        "\n",
        "How to check the data structure?\n",
        "- vars(data)\n",
        "- data.\\__dict__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrTtvgaHZZid",
        "outputId": "48112c3a-b9f8-48b1-95f2-77ede1bb4a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vars()  # 변수 확인 함수"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BucketIterator': torchtext.data.iterator.BucketIterator,\n",
              " 'Field': torchtext.data.field.Field,\n",
              " 'In': ['',\n",
              "  \"get_ipython().system('nvidia-smi')\",\n",
              "  \"from psutil import virtual_memory\\nram_gb = virtual_memory().total / 1e9\\nprint('Your runtime has {:.1f} gigabytes of available RAM\\\\n'.format(ram_gb))\",\n",
              "  \"import time\\nstart = time.time()\\n\\nget_ipython().system('pip install konlpy')\\nget_ipython().system('sudo apt-get install curl git')\\nget_ipython().system('bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)')\\n\\nprint(f'mecab 설치 소요 시간: {((time.time()-start)/60):.2f} min')\",\n",
              "  \"start = time.time()\\n\\nget_ipython().system('python -m spacy download en')\\n\\nprint(f'spacy tokenizer(en) 설치 소요 시간: {((time.time()-start)/60):.2f} min')\",\n",
              "  \"from konlpy.tag import Mecab\\nimport spacy\\n\\nmecab = Mecab()\\nspacy_en = spacy.load('en')\\n\\ndef tokenize_en(text):\\n    return [tok.text for tok in spacy_en.tokenizer(text)]\\n\\nprint('tokenization test with sample texts')\\nprint('tokenizing for Korean with mecab: ', mecab.morphs('안녕하세요 저는 건국대학교에 재학 중인 박도준입니다.'))\\nprint('tokenizing for English: ', tokenize_en('Hello, I am Dojun Park, a student at Konkuk University.'))\",\n",
              "  \"get_ipython().system('pip uninstall torchtext -y')\",\n",
              "  \"get_ipython().system('pip install torchtext')\",\n",
              "  \"from torchtext.data import Field, TabularDataset, BucketIterator\\n\\nkorean = Field(tokenize=mecab.morphs, lower=True, init_token='<sos>', eos_token='<eos>')\\nenglish = Field(tokenize=tokenize_en, lower=True, init_token='<sos>', eos_token='<eos>')\\n\\nfields = {'kor': ('src', korean), 'eng':('trg', english)}\\n\\ntrain_data, valid_data, test_data = TabularDataset.splits(\\n                                                    path = '/content/drive/My Drive/Colab Notebooks',\\n                                                    train = 'train2.csv',\\n                                                    validation = 'valid2.csv',\\n                                                    test = 'test2.csv',\\n                                                    format = 'csv',\\n                                                    fields = fields)\",\n",
              "  'korean.build_vocab(train_data, min_freq=2)\\nenglish.build_vocab(train_data, min_freq=2)',\n",
              "  'print(train_data[0].__dict__)',\n",
              "  'print(valid_data[0].__dict__)',\n",
              "  'print(test_data[0].__dict__)',\n",
              "  'import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\nclass Transformer(nn.Module):\\n    def __init__(\\n        self,\\n        embedding_size,\\n        src_vocab_size,\\n        trg_vocab_size,\\n        src_pad_idx,\\n        num_heads,\\n        num_encoder_layers,\\n        num_decoder_layers,\\n        forward_expansion,\\n        dropout,\\n        max_length,\\n        device):\\n\\n        super(Transformer, self).__init__()\\n        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\\n        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\\n        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\\n        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\\n        self.device = device\\n        self.transformer = nn.Transformer(\\n            embedding_size,\\n            num_heads,\\n            num_encoder_layers,\\n            num_decoder_layers,\\n            forward_expansion,\\n            dropout)\\n        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\\n        self.dropout = nn.Dropout(dropout)\\n        self.src_pad_idx = src_pad_idx\\n\\n    def make_src_mask(self, src):\\n        # src shape: (src_len, N)\\n        src_mask = src.transpose(0, 1) == self.src_pad_idx\\n        # (N, src_len)\\n        return src_mask\\n\\n    def forward(self, src, trg):\\n        src_seq_length, N = src.shape\\n        trg_seq_length, N = trg.shape\\n\\n        src_positions = (\\n            torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N)\\n            .to(self.device)\\n        )\\n\\n        trg_positions = (\\n            torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N)\\n            .to(self.device)\\n        )\\n\\n        embed_src = self.dropout(\\n            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\\n        )\\n\\n        embed_trg = self.dropout(\\n            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\\n        )\\n\\n        src_padding_mask = self.make_src_mask(src)\\n        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\\n            self.device)\\n\\n        out = self.transformer(\\n            embed_src,\\n            embed_trg,\\n            src_key_padding_mask = src_padding_mask,\\n            tgt_mask = trg_mask\\n        )\\n        out = self.fc_out(out)\\n\\n        return out',\n",
              "  \"# Setup the training phase\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nload_model = False\\nsave_model = True\\n\\n# Training hyperparameters\\nnum_epochs = 5\\nlearning_rate = 1e-4\\nbatch_size = 32\\n\\n# Model hyperparameters\\nsrc_vocab_size = len(korean.vocab)\\ntrg_vocab_size = len(english.vocab)\\nembedding_size = 512\\nnum_heads = 8\\nnum_encoder_layers = 6  # in the paper 6\\nnum_decoder_layers = 6\\ndropout = 0.10\\nmax_len = 100\\nforward_expansion = 4\\nsrc_pad_idx = english.vocab.stoi['<pad>']\\n\\n# Tensorboard for nice plots\\nwriter = SummaryWriter('runs/loss_plot')\\nstep = 0\\n\\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\\n    (train_data, valid_data, test_data),\\n    batch_size = batch_size,\\n    sort_within_batch = True,\\n    sort_key = lambda x: len(x.src),\\n    device = device\\n)\\n\\nmodel = Transformer(\\n    embedding_size,\\n    src_vocab_size,\\n    trg_vocab_size,\\n    src_pad_idx,\\n    num_heads,\\n    num_encoder_layers,\\n    num_decoder_layers,\\n    forward_expansion,\\n    dropout,\\n    max_len,\\n    device).to(device)\\n\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\npad_idx = english.vocab.stoi['<pad>']\\ncriterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\",\n",
              "  \"sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\\n\\nfor epoch in range(num_epochs):\\n\\n    model.eval()\\n    model.train()\\n\\n    for batch_idx, batch in enumerate(train_iterator):\\n        inp_data = batch.src.to(device)\\n        target = batch.trg.to(device)\\n\\n        # forward prop\\n        output = model(inp_data, target[:-1])\\n        output = output.reshape(-1, output.shape[2])\\n        target = target[1:].reshape(-1)\\n        optimizer.zero_grad()\\n\\n        loss = criterion(output, target)\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\\n\\n        optimizer.step()\\n        \\n        writer.add_scalar('Training loss', loss, global_step=step)\\n        step += 1\\n\\n\\n    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\\n    translated_sentence = ' '.join(translated_sentence)\\n    translated_sentence = translated_sentence.replace(' ,', ',')\\n    translated_sentence = translated_sentence.replace(' .', '.')\\n    translated_sentence = translated_sentence.replace(' <eos>', '')\\n\\n    print(f'[Epoch] {epoch+1} / {num_epochs}')\\n    print(f'[Loss] {loss:.4f}')\\n    print(f'[Exsample] {sentence} >>> {translated_sentence}')\",\n",
              "  'def translate_sentence(model, sentence, korean, english, device, max_length=50):\\n    \\n    tokens = mecab.morphs(sentence)\\n    # tokens = komoran.morphs(sentence)\\n    # tokens = tokenize_khaiii(sentence)\\n\\n    # Add <SOS> and <EOS> in beginning and end respectively\\n    tokens.insert(0, korean.init_token)\\n    tokens.append(korean.eos_token)\\n\\n    # Go through each korean token and convert to an index\\n    text_to_indices = [korean.vocab.stoi[token] for token in tokens]\\n\\n    # Convert to Tensor\\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\\n\\n    outputs = [english.vocab.stoi[\"<sos>\"]]\\n    for i in range(max_length):\\n        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\\n\\n        with torch.no_grad():\\n            output = model(sentence_tensor, trg_tensor)\\n\\n        best_guess = output.argmax(2)[-1, :].item()\\n        outputs.append(best_guess)\\n\\n        if best_guess == english.vocab.stoi[\"<eos>\"]:\\n            break\\n\\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\\n    # remove start token\\n    return translated_sentence[1:]',\n",
              "  \"sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\\n\\nfor epoch in range(num_epochs):\\n\\n    model.eval()\\n    model.train()\\n\\n    for batch_idx, batch in enumerate(train_iterator):\\n        inp_data = batch.src.to(device)\\n        target = batch.trg.to(device)\\n\\n        # forward prop\\n        output = model(inp_data, target[:-1])\\n        output = output.reshape(-1, output.shape[2])\\n        target = target[1:].reshape(-1)\\n        optimizer.zero_grad()\\n\\n        loss = criterion(output, target)\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\\n\\n        optimizer.step()\\n        \\n        writer.add_scalar('Training loss', loss, global_step=step)\\n        step += 1\\n\\n\\n    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\\n    translated_sentence = ' '.join(translated_sentence)\\n    translated_sentence = translated_sentence.replace(' ,', ',')\\n    translated_sentence = translated_sentence.replace(' .', '.')\\n    translated_sentence = translated_sentence.replace(' <eos>', '')\\n\\n    print(f'[Epoch] {epoch+1} / {num_epochs}')\\n    print(f'[Loss] {loss:.4f}')\\n    print(f'[Exsample] {sentence} >>> {translated_sentence}')\",\n",
              "  'from torchtext.data.metrics import bleu_score\\n\\ndef bleu(data, model, korean, english, device):\\n    targets = []\\n    outputs = []\\n\\n    for example in data:\\n        src = vars(example)[\"src\"]\\n        trg = vars(example)[\"trg\"]\\n\\n        prediction = translate_sentence(model, src, korean, english, device)\\n        prediction = prediction[:-1]  # remove <eos> token\\n\\n        targets.append([trg])\\n        outputs.append(prediction)\\n\\n    return bleu_score(outputs, targets)\\n\\nbleu(test_data, model, korean, english, device)',\n",
              "  'vars()  # 변수 확인 함수'],\n",
              " 'Mecab': konlpy.tag._mecab.Mecab,\n",
              " 'Out': {},\n",
              " 'SummaryWriter': torch.utils.tensorboard.writer.SummaryWriter,\n",
              " 'TabularDataset': torchtext.data.dataset.TabularDataset,\n",
              " 'Transformer': __main__.Transformer,\n",
              " '_': '',\n",
              " '__': '',\n",
              " '___': '',\n",
              " '__builtin__': <module 'builtins' (built-in)>,\n",
              " '__builtins__': <module 'builtins' (built-in)>,\n",
              " '__doc__': 'Automatically created module for IPython interactive environment',\n",
              " '__loader__': None,\n",
              " '__name__': '__main__',\n",
              " '__package__': None,\n",
              " '__spec__': None,\n",
              " '_dh': ['/content'],\n",
              " '_exit_code': 0,\n",
              " '_i': 'from torchtext.data.metrics import bleu_score\\n\\ndef bleu(data, model, korean, english, device):\\n    targets = []\\n    outputs = []\\n\\n    for example in data:\\n        src = vars(example)[\"src\"]\\n        trg = vars(example)[\"trg\"]\\n\\n        prediction = translate_sentence(model, src, korean, english, device)\\n        prediction = prediction[:-1]  # remove <eos> token\\n\\n        targets.append([trg])\\n        outputs.append(prediction)\\n\\n    return bleu_score(outputs, targets)\\n\\nbleu(test_data, model, korean, english, device)',\n",
              " '_i1': '!nvidia-smi',\n",
              " '_i10': 'print(train_data[0].__dict__)',\n",
              " '_i11': 'print(valid_data[0].__dict__)',\n",
              " '_i12': 'print(test_data[0].__dict__)',\n",
              " '_i13': 'import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\nclass Transformer(nn.Module):\\n    def __init__(\\n        self,\\n        embedding_size,\\n        src_vocab_size,\\n        trg_vocab_size,\\n        src_pad_idx,\\n        num_heads,\\n        num_encoder_layers,\\n        num_decoder_layers,\\n        forward_expansion,\\n        dropout,\\n        max_length,\\n        device):\\n\\n        super(Transformer, self).__init__()\\n        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\\n        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\\n        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\\n        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\\n        self.device = device\\n        self.transformer = nn.Transformer(\\n            embedding_size,\\n            num_heads,\\n            num_encoder_layers,\\n            num_decoder_layers,\\n            forward_expansion,\\n            dropout)\\n        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\\n        self.dropout = nn.Dropout(dropout)\\n        self.src_pad_idx = src_pad_idx\\n\\n    def make_src_mask(self, src):\\n        # src shape: (src_len, N)\\n        src_mask = src.transpose(0, 1) == self.src_pad_idx\\n        # (N, src_len)\\n        return src_mask\\n\\n    def forward(self, src, trg):\\n        src_seq_length, N = src.shape\\n        trg_seq_length, N = trg.shape\\n\\n        src_positions = (\\n            torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N)\\n            .to(self.device)\\n        )\\n\\n        trg_positions = (\\n            torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N)\\n            .to(self.device)\\n        )\\n\\n        embed_src = self.dropout(\\n            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\\n        )\\n\\n        embed_trg = self.dropout(\\n            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\\n        )\\n\\n        src_padding_mask = self.make_src_mask(src)\\n        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\\n            self.device)\\n\\n        out = self.transformer(\\n            embed_src,\\n            embed_trg,\\n            src_key_padding_mask = src_padding_mask,\\n            tgt_mask = trg_mask\\n        )\\n        out = self.fc_out(out)\\n\\n        return out',\n",
              " '_i14': \"# Setup the training phase\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nload_model = False\\nsave_model = True\\n\\n# Training hyperparameters\\nnum_epochs = 5\\nlearning_rate = 1e-4\\nbatch_size = 32\\n\\n# Model hyperparameters\\nsrc_vocab_size = len(korean.vocab)\\ntrg_vocab_size = len(english.vocab)\\nembedding_size = 512\\nnum_heads = 8\\nnum_encoder_layers = 6  # in the paper 6\\nnum_decoder_layers = 6\\ndropout = 0.10\\nmax_len = 100\\nforward_expansion = 4\\nsrc_pad_idx = english.vocab.stoi['<pad>']\\n\\n# Tensorboard for nice plots\\nwriter = SummaryWriter('runs/loss_plot')\\nstep = 0\\n\\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\\n    (train_data, valid_data, test_data),\\n    batch_size = batch_size,\\n    sort_within_batch = True,\\n    sort_key = lambda x: len(x.src),\\n    device = device\\n)\\n\\nmodel = Transformer(\\n    embedding_size,\\n    src_vocab_size,\\n    trg_vocab_size,\\n    src_pad_idx,\\n    num_heads,\\n    num_encoder_layers,\\n    num_decoder_layers,\\n    forward_expansion,\\n    dropout,\\n    max_len,\\n    device).to(device)\\n\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\npad_idx = english.vocab.stoi['<pad>']\\ncriterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\",\n",
              " '_i15': \"sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\\n\\nfor epoch in range(num_epochs):\\n\\n    model.eval()\\n    model.train()\\n\\n    for batch_idx, batch in enumerate(train_iterator):\\n        inp_data = batch.src.to(device)\\n        target = batch.trg.to(device)\\n\\n        # forward prop\\n        output = model(inp_data, target[:-1])\\n        output = output.reshape(-1, output.shape[2])\\n        target = target[1:].reshape(-1)\\n        optimizer.zero_grad()\\n\\n        loss = criterion(output, target)\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\\n\\n        optimizer.step()\\n        \\n        writer.add_scalar('Training loss', loss, global_step=step)\\n        step += 1\\n\\n\\n    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\\n    translated_sentence = ' '.join(translated_sentence)\\n    translated_sentence = translated_sentence.replace(' ,', ',')\\n    translated_sentence = translated_sentence.replace(' .', '.')\\n    translated_sentence = translated_sentence.replace(' <eos>', '')\\n\\n    print(f'[Epoch] {epoch+1} / {num_epochs}')\\n    print(f'[Loss] {loss:.4f}')\\n    print(f'[Exsample] {sentence} >>> {translated_sentence}')\",\n",
              " '_i16': 'def translate_sentence(model, sentence, korean, english, device, max_length=50):\\n    \\n    tokens = mecab.morphs(sentence)\\n    # tokens = komoran.morphs(sentence)\\n    # tokens = tokenize_khaiii(sentence)\\n\\n    # Add <SOS> and <EOS> in beginning and end respectively\\n    tokens.insert(0, korean.init_token)\\n    tokens.append(korean.eos_token)\\n\\n    # Go through each korean token and convert to an index\\n    text_to_indices = [korean.vocab.stoi[token] for token in tokens]\\n\\n    # Convert to Tensor\\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\\n\\n    outputs = [english.vocab.stoi[\"<sos>\"]]\\n    for i in range(max_length):\\n        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\\n\\n        with torch.no_grad():\\n            output = model(sentence_tensor, trg_tensor)\\n\\n        best_guess = output.argmax(2)[-1, :].item()\\n        outputs.append(best_guess)\\n\\n        if best_guess == english.vocab.stoi[\"<eos>\"]:\\n            break\\n\\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\\n    # remove start token\\n    return translated_sentence[1:]',\n",
              " '_i17': \"sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\\n\\nfor epoch in range(num_epochs):\\n\\n    model.eval()\\n    model.train()\\n\\n    for batch_idx, batch in enumerate(train_iterator):\\n        inp_data = batch.src.to(device)\\n        target = batch.trg.to(device)\\n\\n        # forward prop\\n        output = model(inp_data, target[:-1])\\n        output = output.reshape(-1, output.shape[2])\\n        target = target[1:].reshape(-1)\\n        optimizer.zero_grad()\\n\\n        loss = criterion(output, target)\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\\n\\n        optimizer.step()\\n        \\n        writer.add_scalar('Training loss', loss, global_step=step)\\n        step += 1\\n\\n\\n    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\\n    translated_sentence = ' '.join(translated_sentence)\\n    translated_sentence = translated_sentence.replace(' ,', ',')\\n    translated_sentence = translated_sentence.replace(' .', '.')\\n    translated_sentence = translated_sentence.replace(' <eos>', '')\\n\\n    print(f'[Epoch] {epoch+1} / {num_epochs}')\\n    print(f'[Loss] {loss:.4f}')\\n    print(f'[Exsample] {sentence} >>> {translated_sentence}')\",\n",
              " '_i18': 'from torchtext.data.metrics import bleu_score\\n\\ndef bleu(data, model, korean, english, device):\\n    targets = []\\n    outputs = []\\n\\n    for example in data:\\n        src = vars(example)[\"src\"]\\n        trg = vars(example)[\"trg\"]\\n\\n        prediction = translate_sentence(model, src, korean, english, device)\\n        prediction = prediction[:-1]  # remove <eos> token\\n\\n        targets.append([trg])\\n        outputs.append(prediction)\\n\\n    return bleu_score(outputs, targets)\\n\\nbleu(test_data, model, korean, english, device)',\n",
              " '_i19': 'vars()  # 변수 확인 함수',\n",
              " '_i2': \"from psutil import virtual_memory\\nram_gb = virtual_memory().total / 1e9\\nprint('Your runtime has {:.1f} gigabytes of available RAM\\\\n'.format(ram_gb))\",\n",
              " '_i3': \"import time\\nstart = time.time()\\n\\n!pip install konlpy\\n!sudo apt-get install curl git\\n!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\\n\\nprint(f'mecab 설치 소요 시간: {((time.time()-start)/60):.2f} min')\",\n",
              " '_i4': \"start = time.time()\\n\\n!python -m spacy download en\\n\\nprint(f'spacy tokenizer(en) 설치 소요 시간: {((time.time()-start)/60):.2f} min')\",\n",
              " '_i5': \"from konlpy.tag import Mecab\\nimport spacy\\n\\nmecab = Mecab()\\nspacy_en = spacy.load('en')\\n\\ndef tokenize_en(text):\\n    return [tok.text for tok in spacy_en.tokenizer(text)]\\n\\nprint('tokenization test with sample texts')\\nprint('tokenizing for Korean with mecab: ', mecab.morphs('안녕하세요 저는 건국대학교에 재학 중인 박도준입니다.'))\\nprint('tokenizing for English: ', tokenize_en('Hello, I am Dojun Park, a student at Konkuk University.'))\",\n",
              " '_i6': '!pip uninstall torchtext -y',\n",
              " '_i7': '!pip install torchtext',\n",
              " '_i8': \"from torchtext.data import Field, TabularDataset, BucketIterator\\n\\nkorean = Field(tokenize=mecab.morphs, lower=True, init_token='<sos>', eos_token='<eos>')\\nenglish = Field(tokenize=tokenize_en, lower=True, init_token='<sos>', eos_token='<eos>')\\n\\nfields = {'kor': ('src', korean), 'eng':('trg', english)}\\n\\ntrain_data, valid_data, test_data = TabularDataset.splits(\\n                                                    path = '/content/drive/My Drive/Colab Notebooks',\\n                                                    train = 'train2.csv',\\n                                                    validation = 'valid2.csv',\\n                                                    test = 'test2.csv',\\n                                                    format = 'csv',\\n                                                    fields = fields)\",\n",
              " '_i9': 'korean.build_vocab(train_data, min_freq=2)\\nenglish.build_vocab(train_data, min_freq=2)',\n",
              " '_ih': ['',\n",
              "  \"get_ipython().system('nvidia-smi')\",\n",
              "  \"from psutil import virtual_memory\\nram_gb = virtual_memory().total / 1e9\\nprint('Your runtime has {:.1f} gigabytes of available RAM\\\\n'.format(ram_gb))\",\n",
              "  \"import time\\nstart = time.time()\\n\\nget_ipython().system('pip install konlpy')\\nget_ipython().system('sudo apt-get install curl git')\\nget_ipython().system('bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)')\\n\\nprint(f'mecab 설치 소요 시간: {((time.time()-start)/60):.2f} min')\",\n",
              "  \"start = time.time()\\n\\nget_ipython().system('python -m spacy download en')\\n\\nprint(f'spacy tokenizer(en) 설치 소요 시간: {((time.time()-start)/60):.2f} min')\",\n",
              "  \"from konlpy.tag import Mecab\\nimport spacy\\n\\nmecab = Mecab()\\nspacy_en = spacy.load('en')\\n\\ndef tokenize_en(text):\\n    return [tok.text for tok in spacy_en.tokenizer(text)]\\n\\nprint('tokenization test with sample texts')\\nprint('tokenizing for Korean with mecab: ', mecab.morphs('안녕하세요 저는 건국대학교에 재학 중인 박도준입니다.'))\\nprint('tokenizing for English: ', tokenize_en('Hello, I am Dojun Park, a student at Konkuk University.'))\",\n",
              "  \"get_ipython().system('pip uninstall torchtext -y')\",\n",
              "  \"get_ipython().system('pip install torchtext')\",\n",
              "  \"from torchtext.data import Field, TabularDataset, BucketIterator\\n\\nkorean = Field(tokenize=mecab.morphs, lower=True, init_token='<sos>', eos_token='<eos>')\\nenglish = Field(tokenize=tokenize_en, lower=True, init_token='<sos>', eos_token='<eos>')\\n\\nfields = {'kor': ('src', korean), 'eng':('trg', english)}\\n\\ntrain_data, valid_data, test_data = TabularDataset.splits(\\n                                                    path = '/content/drive/My Drive/Colab Notebooks',\\n                                                    train = 'train2.csv',\\n                                                    validation = 'valid2.csv',\\n                                                    test = 'test2.csv',\\n                                                    format = 'csv',\\n                                                    fields = fields)\",\n",
              "  'korean.build_vocab(train_data, min_freq=2)\\nenglish.build_vocab(train_data, min_freq=2)',\n",
              "  'print(train_data[0].__dict__)',\n",
              "  'print(valid_data[0].__dict__)',\n",
              "  'print(test_data[0].__dict__)',\n",
              "  'import torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\nclass Transformer(nn.Module):\\n    def __init__(\\n        self,\\n        embedding_size,\\n        src_vocab_size,\\n        trg_vocab_size,\\n        src_pad_idx,\\n        num_heads,\\n        num_encoder_layers,\\n        num_decoder_layers,\\n        forward_expansion,\\n        dropout,\\n        max_length,\\n        device):\\n\\n        super(Transformer, self).__init__()\\n        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\\n        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\\n        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\\n        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\\n        self.device = device\\n        self.transformer = nn.Transformer(\\n            embedding_size,\\n            num_heads,\\n            num_encoder_layers,\\n            num_decoder_layers,\\n            forward_expansion,\\n            dropout)\\n        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\\n        self.dropout = nn.Dropout(dropout)\\n        self.src_pad_idx = src_pad_idx\\n\\n    def make_src_mask(self, src):\\n        # src shape: (src_len, N)\\n        src_mask = src.transpose(0, 1) == self.src_pad_idx\\n        # (N, src_len)\\n        return src_mask\\n\\n    def forward(self, src, trg):\\n        src_seq_length, N = src.shape\\n        trg_seq_length, N = trg.shape\\n\\n        src_positions = (\\n            torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N)\\n            .to(self.device)\\n        )\\n\\n        trg_positions = (\\n            torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N)\\n            .to(self.device)\\n        )\\n\\n        embed_src = self.dropout(\\n            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\\n        )\\n\\n        embed_trg = self.dropout(\\n            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\\n        )\\n\\n        src_padding_mask = self.make_src_mask(src)\\n        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\\n            self.device)\\n\\n        out = self.transformer(\\n            embed_src,\\n            embed_trg,\\n            src_key_padding_mask = src_padding_mask,\\n            tgt_mask = trg_mask\\n        )\\n        out = self.fc_out(out)\\n\\n        return out',\n",
              "  \"# Setup the training phase\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nload_model = False\\nsave_model = True\\n\\n# Training hyperparameters\\nnum_epochs = 5\\nlearning_rate = 1e-4\\nbatch_size = 32\\n\\n# Model hyperparameters\\nsrc_vocab_size = len(korean.vocab)\\ntrg_vocab_size = len(english.vocab)\\nembedding_size = 512\\nnum_heads = 8\\nnum_encoder_layers = 6  # in the paper 6\\nnum_decoder_layers = 6\\ndropout = 0.10\\nmax_len = 100\\nforward_expansion = 4\\nsrc_pad_idx = english.vocab.stoi['<pad>']\\n\\n# Tensorboard for nice plots\\nwriter = SummaryWriter('runs/loss_plot')\\nstep = 0\\n\\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\\n    (train_data, valid_data, test_data),\\n    batch_size = batch_size,\\n    sort_within_batch = True,\\n    sort_key = lambda x: len(x.src),\\n    device = device\\n)\\n\\nmodel = Transformer(\\n    embedding_size,\\n    src_vocab_size,\\n    trg_vocab_size,\\n    src_pad_idx,\\n    num_heads,\\n    num_encoder_layers,\\n    num_decoder_layers,\\n    forward_expansion,\\n    dropout,\\n    max_len,\\n    device).to(device)\\n\\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\\npad_idx = english.vocab.stoi['<pad>']\\ncriterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\",\n",
              "  \"sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\\n\\nfor epoch in range(num_epochs):\\n\\n    model.eval()\\n    model.train()\\n\\n    for batch_idx, batch in enumerate(train_iterator):\\n        inp_data = batch.src.to(device)\\n        target = batch.trg.to(device)\\n\\n        # forward prop\\n        output = model(inp_data, target[:-1])\\n        output = output.reshape(-1, output.shape[2])\\n        target = target[1:].reshape(-1)\\n        optimizer.zero_grad()\\n\\n        loss = criterion(output, target)\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\\n\\n        optimizer.step()\\n        \\n        writer.add_scalar('Training loss', loss, global_step=step)\\n        step += 1\\n\\n\\n    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\\n    translated_sentence = ' '.join(translated_sentence)\\n    translated_sentence = translated_sentence.replace(' ,', ',')\\n    translated_sentence = translated_sentence.replace(' .', '.')\\n    translated_sentence = translated_sentence.replace(' <eos>', '')\\n\\n    print(f'[Epoch] {epoch+1} / {num_epochs}')\\n    print(f'[Loss] {loss:.4f}')\\n    print(f'[Exsample] {sentence} >>> {translated_sentence}')\",\n",
              "  'def translate_sentence(model, sentence, korean, english, device, max_length=50):\\n    \\n    tokens = mecab.morphs(sentence)\\n    # tokens = komoran.morphs(sentence)\\n    # tokens = tokenize_khaiii(sentence)\\n\\n    # Add <SOS> and <EOS> in beginning and end respectively\\n    tokens.insert(0, korean.init_token)\\n    tokens.append(korean.eos_token)\\n\\n    # Go through each korean token and convert to an index\\n    text_to_indices = [korean.vocab.stoi[token] for token in tokens]\\n\\n    # Convert to Tensor\\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\\n\\n    outputs = [english.vocab.stoi[\"<sos>\"]]\\n    for i in range(max_length):\\n        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\\n\\n        with torch.no_grad():\\n            output = model(sentence_tensor, trg_tensor)\\n\\n        best_guess = output.argmax(2)[-1, :].item()\\n        outputs.append(best_guess)\\n\\n        if best_guess == english.vocab.stoi[\"<eos>\"]:\\n            break\\n\\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\\n    # remove start token\\n    return translated_sentence[1:]',\n",
              "  \"sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\\n\\nfor epoch in range(num_epochs):\\n\\n    model.eval()\\n    model.train()\\n\\n    for batch_idx, batch in enumerate(train_iterator):\\n        inp_data = batch.src.to(device)\\n        target = batch.trg.to(device)\\n\\n        # forward prop\\n        output = model(inp_data, target[:-1])\\n        output = output.reshape(-1, output.shape[2])\\n        target = target[1:].reshape(-1)\\n        optimizer.zero_grad()\\n\\n        loss = criterion(output, target)\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\\n\\n        optimizer.step()\\n        \\n        writer.add_scalar('Training loss', loss, global_step=step)\\n        step += 1\\n\\n\\n    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\\n    translated_sentence = ' '.join(translated_sentence)\\n    translated_sentence = translated_sentence.replace(' ,', ',')\\n    translated_sentence = translated_sentence.replace(' .', '.')\\n    translated_sentence = translated_sentence.replace(' <eos>', '')\\n\\n    print(f'[Epoch] {epoch+1} / {num_epochs}')\\n    print(f'[Loss] {loss:.4f}')\\n    print(f'[Exsample] {sentence} >>> {translated_sentence}')\",\n",
              "  'from torchtext.data.metrics import bleu_score\\n\\ndef bleu(data, model, korean, english, device):\\n    targets = []\\n    outputs = []\\n\\n    for example in data:\\n        src = vars(example)[\"src\"]\\n        trg = vars(example)[\"trg\"]\\n\\n        prediction = translate_sentence(model, src, korean, english, device)\\n        prediction = prediction[:-1]  # remove <eos> token\\n\\n        targets.append([trg])\\n        outputs.append(prediction)\\n\\n    return bleu_score(outputs, targets)\\n\\nbleu(test_data, model, korean, english, device)',\n",
              "  'vars()  # 변수 확인 함수'],\n",
              " '_ii': \"sentence = '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.'  # df['kor'][117]\\n\\nfor epoch in range(num_epochs):\\n\\n    model.eval()\\n    model.train()\\n\\n    for batch_idx, batch in enumerate(train_iterator):\\n        inp_data = batch.src.to(device)\\n        target = batch.trg.to(device)\\n\\n        # forward prop\\n        output = model(inp_data, target[:-1])\\n        output = output.reshape(-1, output.shape[2])\\n        target = target[1:].reshape(-1)\\n        optimizer.zero_grad()\\n\\n        loss = criterion(output, target)\\n        loss.backward()\\n\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\\n\\n        optimizer.step()\\n        \\n        writer.add_scalar('Training loss', loss, global_step=step)\\n        step += 1\\n\\n\\n    translated_sentence = translate_sentence(model, sentence, korean, english, device, max_length = 100)\\n    translated_sentence = ' '.join(translated_sentence)\\n    translated_sentence = translated_sentence.replace(' ,', ',')\\n    translated_sentence = translated_sentence.replace(' .', '.')\\n    translated_sentence = translated_sentence.replace(' <eos>', '')\\n\\n    print(f'[Epoch] {epoch+1} / {num_epochs}')\\n    print(f'[Loss] {loss:.4f}')\\n    print(f'[Exsample] {sentence} >>> {translated_sentence}')\",\n",
              " '_iii': 'def translate_sentence(model, sentence, korean, english, device, max_length=50):\\n    \\n    tokens = mecab.morphs(sentence)\\n    # tokens = komoran.morphs(sentence)\\n    # tokens = tokenize_khaiii(sentence)\\n\\n    # Add <SOS> and <EOS> in beginning and end respectively\\n    tokens.insert(0, korean.init_token)\\n    tokens.append(korean.eos_token)\\n\\n    # Go through each korean token and convert to an index\\n    text_to_indices = [korean.vocab.stoi[token] for token in tokens]\\n\\n    # Convert to Tensor\\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\\n\\n    outputs = [english.vocab.stoi[\"<sos>\"]]\\n    for i in range(max_length):\\n        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\\n\\n        with torch.no_grad():\\n            output = model(sentence_tensor, trg_tensor)\\n\\n        best_guess = output.argmax(2)[-1, :].item()\\n        outputs.append(best_guess)\\n\\n        if best_guess == english.vocab.stoi[\"<eos>\"]:\\n            break\\n\\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\\n    # remove start token\\n    return translated_sentence[1:]',\n",
              " '_oh': {},\n",
              " '_sh': <module 'IPython.core.shadowns' from '/usr/local/lib/python3.6/dist-packages/IPython/core/shadowns.py'>,\n",
              " 'batch': \n",
              " [torchtext.data.batch.Batch of size 32]\n",
              " \t[.src]:[torch.cuda.LongTensor of size 24x32 (GPU 0)]\n",
              " \t[.trg]:[torch.cuda.LongTensor of size 28x32 (GPU 0)],\n",
              " 'batch_idx': 562,\n",
              " 'batch_size': 32,\n",
              " 'bleu': <function __main__.bleu>,\n",
              " 'bleu_score': <function torchtext.data.metrics.bleu_score>,\n",
              " 'criterion': CrossEntropyLoss(),\n",
              " 'device': device(type='cuda'),\n",
              " 'dropout': 0.1,\n",
              " 'embedding_size': 512,\n",
              " 'english': <torchtext.data.field.Field at 0x7f9583a2c1d0>,\n",
              " 'epoch': 4,\n",
              " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f95e2729898>,\n",
              " 'fields': {'eng': ('trg', <torchtext.data.field.Field at 0x7f9583a2c1d0>),\n",
              "  'kor': ('src', <torchtext.data.field.Field at 0x7f9583a2c160>)},\n",
              " 'forward_expansion': 4,\n",
              " 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7f95e27162e8>>,\n",
              " 'inp_data': tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
              "              2,     2],\n",
              "         [  217,   518,   967,     0,     4,    69,    89,  3223,    93,    89,\n",
              "           7176,  1133,  3465,  8368,    89,  5921,     0,   460,   950,  4607,\n",
              "           1042,    89,   115,  8493,   487,   591,   868,  7408,  4614,    64,\n",
              "           2709,   628],\n",
              "         [   19,   494,  4895,  2696,  4068,     4,     9,    10,  2771,     9,\n",
              "           6852,    27,    18,    82,     9,  8585,  1643,   368,   280,     0,\n",
              "              7,     7,  7866,     8,    85,   316,   917,    24,   356,    11,\n",
              "             18,    18],\n",
              "         [  187,     9,  8624,  4329,  2700,  5042,  1180,     9,    13,   426,\n",
              "           1589,    19,    13,    71,   945,  1919,  7360,    25,     7,  6107,\n",
              "           4825,     4,     9,   195,   432,   993,    11,  1159,     4,   536,\n",
              "             11,    13],\n",
              "         [  185,   218,  7553,    10,    26,   102,  4438,    43,  1596,   247,\n",
              "             10,  3463,   155,    64,  2979,  9335,    13,    68,    93,    26,\n",
              "           5863,    15,  1601,    12,   240,    10,   983,   982,  2799,    10,\n",
              "           1577,   574],\n",
              "         [    9,  1482,    23,     7,  8981,  4363,    15,   897,    33,    83,\n",
              "              7,   280,    11,     4,     8,     4,  1632,    43,   400,  8661,\n",
              "             15,   136,  1891,   751,    24,     7,    15,   365,    79,  5113,\n",
              "              9,    27],\n",
              "         [  546,    64, 10419,  1822,  8615,     8,  1458,    10, 13173, 12378,\n",
              "           1985,     7,  6018,   411,   241,   449,  2568,   908,  1104,     0,\n",
              "           3547,    83,    15,     4,   291,    84,  5691,   185,   149,    12,\n",
              "            862,    19],\n",
              "         [ 1299,    11,     0,     8,  6596,  4304,    12,   428,     9,  1130,\n",
              "            687,  9490,     4,    17,    55,    18,   766,  1127,    25,     8,\n",
              "           7334,    46,   482,    18,    52,     4,    16,     9,    43,    29,\n",
              "             56,   798],\n",
              "         [  326,  1034,  3945,   241,    35,    21,    76,     7,  1659,  1010,\n",
              "            803,  1684, 10649,  1395,  1133,    11,     9,    33,  5547,   352,\n",
              "             12,   348,    40,    97,  9273,   705,   448,   982,     0,    38,\n",
              "             23,     9],\n",
              "         [   33,     8,    11,    13,    52,    16,  4225,    28,   747,   102,\n",
              "           4394,    25,    33,  4459,    64,   319,   170,  1206,    33,    12,\n",
              "             29,  1108,    68,     7,  1318,    17,    13,  5429,     4,     7,\n",
              "              0,  3360],\n",
              "         [  422,     0,     0,   488,   540,    23,   583,   937,   997,    15,\n",
              "            204,   491,   840,     8,    98,     8,   682,  5145,   158,    43,\n",
              "             38,   105,    16,  3519,    25,   179,   379,     8,   526,   426,\n",
              "           5480,    98],\n",
              "         [ 1247,   619,   829,    18,  5459,     4,     8,    11,   425,   119,\n",
              "            107,     8,    12,  3671,   120,  1712,    58,    32,    47,  3533,\n",
              "             16,     4,  8343,  5378,  4115,    18,     4,  4705,  1140,   219,\n",
              "             11,  1152],\n",
              "         [ 2650,    27,  7058,     4,    64,    15,   219,     0,  1235,    84,\n",
              "            112,   409,    41,    21,   417,   584,   501,   158,   192,    71,\n",
              "            106,  1564,    40,     8,    17,     8,    44,   365,     8,     4,\n",
              "            754,   432],\n",
              "         [   15,    32,  4895,  2696,    10,  1902,    12,     8,     0,  1253,\n",
              "             11,    37,   111,    52,  8493,     7,     8,   223,   851,   419,\n",
              "            204,   750,    68,    68,  5604,  2409,    36,    12,  4522,   129,\n",
              "           7764,     8],\n",
              "         [ 1762,   145,   967,    12,  1710,     7,     7,   999,    10,  3264,\n",
              "            179,   307,     8,    20,     8,   889,  3724,  1889,    58,   504,\n",
              "            107,    90,     7,   751,     9,    37,    52,     7,     7,    19,\n",
              "              7,  1902],\n",
              "         [   24,   594,  1517,    30,   112,    20,   180,    79,  6409,     8,\n",
              "           9802,    18,  2464,    26,     0,    17,    34,     4,     4, 11213,\n",
              "              4,    20,    20,    77,   196,   524,    20,   767,  1680,   706,\n",
              "             84,     7],\n",
              "         [  398,    19,     9,   118,    84,   257,     9,   184,    12,   882,\n",
              "             84,     4,    37,  2630,   204,  5034,    20,   126,  1349,    10,\n",
              "           1901,    19,     4,    18,    40,    13,    22,     7,    27,    90,\n",
              "            597,    20],\n",
              "         [   34,   992,   984,    13,    18,   191,   504,    41,    56,    12,\n",
              "             18,    79,     0,    27,   107,   486,     4,   503,    49,  1186,\n",
              "              0,   166,  3513,     4,    29,  1762,     0, 12379,     0,    73,\n",
              "             10,     8],\n",
              "         [   31,    12,  2694,  2864,     4,    30,   123,   516,   413,    30,\n",
              "              4,   463,     4,    32,   210,     8,    99,    19,    57,     4,\n",
              "           2229,    12,     4,  1863,     7,    18,    82,     9,     8,    58,\n",
              "              0,  4288],\n",
              "         [   14,    16,    19,     4,   457,    60,    25,    18,     4,    25,\n",
              "           1664,   757,    99,   133,    25,   154,   430,  9168,   230,    73,\n",
              "             25,    16,    99,    28,    20,     4,    24,   177,   431,    13,\n",
              "              8,    29],\n",
              "         [   28,    14,   353,    59,    21,     4,     0,     4,  2094,    21,\n",
              "             14,     4, 11230,     4,  1062,    21,    21,    49,    21,    12,\n",
              "           5311,    14,   233,  6860,     4,   867,   175,     4,    28,   284,\n",
              "           6074,    38],\n",
              "         [   50,     6,    50,     6,     6,     6,     6,     6,     6,     6,\n",
              "              6,     6,     6,     6,    42,     6,     6,     6,     6,     6,\n",
              "             50,     6,     6,   323,     6,    42,     6,     6,  1246,     6,\n",
              "              6,   199],\n",
              "         [    5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
              "              5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
              "              5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n",
              "              5,     5],\n",
              "         [    3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
              "              3,     3]], device='cuda:0'),\n",
              " 'korean': <torchtext.data.field.Field at 0x7f9583a2c160>,\n",
              " 'learning_rate': 0.0001,\n",
              " 'load_model': False,\n",
              " 'loss': tensor(4.9534, device='cuda:0', grad_fn=<NllLossBackward>),\n",
              " 'max_len': 100,\n",
              " 'mecab': <konlpy.tag._mecab.Mecab at 0x7f95dbe41208>,\n",
              " 'model': Transformer(\n",
              "   (src_word_embedding): Embedding(13578, 512)\n",
              "   (src_position_embedding): Embedding(100, 512)\n",
              "   (trg_word_embedding): Embedding(11062, 512)\n",
              "   (trg_position_embedding): Embedding(100, 512)\n",
              "   (transformer): Transformer(\n",
              "     (encoder): TransformerEncoder(\n",
              "       (layers): ModuleList(\n",
              "         (0): TransformerEncoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (1): TransformerEncoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (2): TransformerEncoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (3): TransformerEncoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (4): TransformerEncoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (5): TransformerEncoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "       )\n",
              "       (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "     )\n",
              "     (decoder): TransformerDecoder(\n",
              "       (layers): ModuleList(\n",
              "         (0): TransformerDecoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (multihead_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "           (dropout3): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (1): TransformerDecoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (multihead_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "           (dropout3): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (2): TransformerDecoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (multihead_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "           (dropout3): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (3): TransformerDecoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (multihead_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "           (dropout3): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (4): TransformerDecoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (multihead_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "           (dropout3): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "         (5): TransformerDecoderLayer(\n",
              "           (self_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (multihead_attn): MultiheadAttention(\n",
              "             (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "           )\n",
              "           (linear1): Linear(in_features=512, out_features=4, bias=True)\n",
              "           (dropout): Dropout(p=0.1, inplace=False)\n",
              "           (linear2): Linear(in_features=4, out_features=512, bias=True)\n",
              "           (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout1): Dropout(p=0.1, inplace=False)\n",
              "           (dropout2): Dropout(p=0.1, inplace=False)\n",
              "           (dropout3): Dropout(p=0.1, inplace=False)\n",
              "         )\n",
              "       )\n",
              "       (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "     )\n",
              "   )\n",
              "   (fc_out): Linear(in_features=512, out_features=11062, bias=True)\n",
              "   (dropout): Dropout(p=0.1, inplace=False)\n",
              " ),\n",
              " 'nn': <module 'torch.nn' from '/usr/local/lib/python3.6/dist-packages/torch/nn/__init__.py'>,\n",
              " 'num_decoder_layers': 6,\n",
              " 'num_encoder_layers': 6,\n",
              " 'num_epochs': 5,\n",
              " 'num_heads': 8,\n",
              " 'optim': <module 'torch.optim' from '/usr/local/lib/python3.6/dist-packages/torch/optim/__init__.py'>,\n",
              " 'optimizer': Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     lr: 0.0001\n",
              "     weight_decay: 0\n",
              " ),\n",
              " 'output': tensor([[ 6.0036, -4.1533, -6.5215,  ..., -2.4360, -1.4260, -1.7343],\n",
              "         [ 5.9832, -4.7449, -6.4115,  ..., -1.8728, -2.0698, -2.3668],\n",
              "         [ 6.6812, -3.4744, -5.8264,  ..., -1.5535, -1.0619, -1.0311],\n",
              "         ...,\n",
              "         [ 4.3964, -3.6757, -5.7807,  ..., -2.1603, -2.3249, -2.7020],\n",
              "         [ 4.9296, -4.8000, -6.9543,  ..., -2.4472, -2.7034, -2.7128],\n",
              "         [ 4.2187, -4.7432, -7.0614,  ..., -2.7215, -2.4531, -3.1348]],\n",
              "        device='cuda:0', grad_fn=<ViewBackward>),\n",
              " 'pad_idx': 1,\n",
              " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f95e2729898>,\n",
              " 'ram_gb': 13.653540864,\n",
              " 'save_model': True,\n",
              " 'sentence': '이 지역의 많은 공장들이 다른 곳에 외주를 주고 있다.',\n",
              " 'spacy': <module 'spacy' from '/usr/local/lib/python3.6/dist-packages/spacy/__init__.py'>,\n",
              " 'spacy_en': <spacy.lang.en.English at 0x7f9583a2b5f8>,\n",
              " 'src_pad_idx': 1,\n",
              " 'src_vocab_size': 13578,\n",
              " 'start': 1603507458.3837883,\n",
              " 'step': 2847,\n",
              " 'target': tensor([    9,   901,     4,     0,   318,    14,    41,    31,    22,     4,\n",
              "             4,  6693,  4596,    14,     9,  8022,  1724,    28,     4,     4,\n",
              "            33,     4,   257,    49,  4604,  2089,    15,  1285,    47,    15,\n",
              "            20,   445,     4,   758,   717,    13,    15,    16,     4,    17,\n",
              "          5148,    67,     0,     6,   422,    16,  5967,  3049,  2607,   447,\n",
              "           531,  2343,    22,    67,    60,    57,  1409,     6,    18,   844,\n",
              "             4,    11,   514,  3007,   273,    11,  4833,   187,    84,    50,\n",
              "            67,    12,    18,    26,  1132,     4,    15,     9,     7,    60,\n",
              "            13,     6,   112,  2162,    48,  4067,   316,    27,    17,    31,\n",
              "            32,  1175,  3196,  1872,    68,    82,     6,   740,  4128,  8715,\n",
              "           441,     4,  1894,   255,  1284,   356,  5194,     0,    12,  2169,\n",
              "             4,    26,  3222,     4,  3660,  2665,     6,    14,    13,   409,\n",
              "           668,    17,     9,    11,  1530,    14,  8493,    30,   144,    20,\n",
              "          4563,  1734,     9,    85,    12,     7,    20,     8,   348,   112,\n",
              "          6934,     8,  3444,   226,     6,   214,    14,  2918,   146,  5830,\n",
              "           303,     8,    24,  5230, 11020,     4,  1559,   650,   615,   115,\n",
              "            84,  1485,     6,    26,     4,    26,  1188,  1272,  1050,  2667,\n",
              "           161,  1519,    50,     4,    66,    12,   538,  1393,     4,    10,\n",
              "          2482,    66,    59,   227,  3363,     9,    90,    52,    62,    10,\n",
              "             6,    53,    27,   176,  1402,    12,   487,  4547,  9029,    14,\n",
              "            13,   548,  2125,  6465,     4,     0,    38,   327,     7,    19,\n",
              "          1933,     0,   218,   514,   352,     4,    13,   197,    12,   312,\n",
              "             6,   286,     0,   148,    25,     6,     0,   255,   955,    12,\n",
              "             6,  1733,    82,  3769,    84,   175,   272,     8,     6,   399,\n",
              "             4,   388,  1125,   354,    27,    27,   991,  1153,  3943,  1099,\n",
              "           606,    90,    62,    14,  3787,     8,   491,   448,  2163,     7,\n",
              "            58,  3235,   799,    62,    10,  3248,    24,   266,    16,     4,\n",
              "             4,   383,  2435,    13,    26,    36,    25,    25,     8,     7,\n",
              "          1984,    14,   431,    12,    27,    82,    16,   326,     8,  1042,\n",
              "          2424,   125,     6,     9,    84,     4, 10067,     9,    37,    21,\n",
              "           896,  2917,    67,    14,   361,    72,  3614,   199,  2338,  2842,\n",
              "           199,     4,    34,  2815,    97,   197,    29,    30,   182,  4444,\n",
              "          3828,     7, 11049,  1528,    40,     4,   603,  3072,    13,   366,\n",
              "           720,   216,  1250,   208,   909,  2295,     6,  5087,  3292,    62,\n",
              "             8,     5,   100,    58,  3039,    84,   285,   844,  3083,   745,\n",
              "           836,    10,  1228,    58,  4833,    90,    26,  6321,  1909,     7,\n",
              "           572,     8,  1238,  3514,   115,    14,  1165,   418,  1126,    10,\n",
              "           211,     8,     4,     3,  1767,    56,    10,    68,    16,  3735,\n",
              "             0,    58,   192,   593,   586,  7497,   717,  1216,    44,  1276,\n",
              "          1889,   131,   104,  2310,   172,     6,    12,    58,    58,    68,\n",
              "            14,   161,     8,  3306,     0,     1,    16,   382,   404,  1657,\n",
              "          2572,  6005,  1308,   762,   493,   404,   309,     9,  2739,    36,\n",
              "         10386,     6,  1795,  1085,     0,     0,     7,    24,  6934,  3961,\n",
              "            10,  1622,  3022,   119,  8081,     9,   785,     1,   291,    39,\n",
              "             5,     5,  1511,    37,  1000,    27,    12,     5,    10,   740,\n",
              "            11,   741,    28,    10,     5,     5,     8,   388,   146,   245,\n",
              "             5,   331,   170,     5,  1854,   850,     5,    64,   136,     1,\n",
              "            10,   409,     3,     3,    80,   844,     7,   444,    66,     3,\n",
              "           294,  2511,  2428,  2100,  3954,   280,     3,     3,     4,    13,\n",
              "          1265,   131,     3,  1265,   413,     3,  1784,   193,     3,   799,\n",
              "         11016,     1,   295,     8,     1,     1,   186,  4537,     0,     8,\n",
              "          4499,     1,   562,     5,    20,    33,    10,     8,     1,     1,\n",
              "          2217,  1534,   325,     4,     1,   268,  2183,     1,   153,   837,\n",
              "             1,   933,  1479,     1, 10640,   224,     1,     1,     5,     9,\n",
              "             5,   119,     8,     1,  3448,     3,    12,     4,  5329,   741,\n",
              "             1,     1,  1721,     9,     5,   305,     1,   786,  4643,     1,\n",
              "           214,     5,     1,     5,   315,     1,    15,     4,     1,     1,\n",
              "             3,  6299,     3,     5,   548,     1,     9,     1,   969,   194,\n",
              "             5,    15,     1,     1,     0,     4,     3,  1824,     1,     5,\n",
              "          1236,     1,   201,     3,     1,     3,   324,     1,    65,  2183,\n",
              "             1,     1,     1,     5,     1,     3,   168,     1,     4,     1,\n",
              "          3229,     7,     3,     5,     1,     1,     7,   650,     1,   800,\n",
              "             1,     3,     0,     1,  1147,     1,     1,     1,     5,     1,\n",
              "             5,  4247,     1,     1,     1,     3,     1,     1,     5,     1,\n",
              "          1678,     1,     5,     4,     1,     3,     1,     1,     4,   112,\n",
              "             1,     5,     1,     1,   325,     1,   165,     1,     1,     1,\n",
              "             3,     1,     3,     5,     1,     1,     1,     1,     1,     1,\n",
              "             3,     1,     5,     1,     3,    38,     1,     1,     1,     1,\n",
              "            67,     5,     1,     3,     1,     1,   124,     1,     5,     1,\n",
              "             1,     1,     1,     1,     1,     3,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     3,     1,     1,     5,     1,     1,\n",
              "             1,     1,   208,     3,     1,     1,     1,     1,     5,     1,\n",
              "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     3,\n",
              "             1,     1,     1,     1,  5252,     1,     1,     1,     1,     1,\n",
              "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,   361,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     5,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1], device='cuda:0'),\n",
              " 'test_data': <torchtext.data.dataset.TabularDataset at 0x7f9583a2c278>,\n",
              " 'test_iterator': <torchtext.data.iterator.BucketIterator at 0x7f952ad97080>,\n",
              " 'time': <module 'time' (built-in)>,\n",
              " 'tokenize_en': <function __main__.tokenize_en>,\n",
              " 'torch': <module 'torch' from '/usr/local/lib/python3.6/dist-packages/torch/__init__.py'>,\n",
              " 'train_data': <torchtext.data.dataset.TabularDataset at 0x7f9583a2c208>,\n",
              " 'train_iterator': <torchtext.data.iterator.BucketIterator at 0x7f952ad91630>,\n",
              " 'translate_sentence': <function __main__.translate_sentence>,\n",
              " 'translated_sentence': 'many people are many of the country of the <unk> are many.',\n",
              " 'trg_vocab_size': 11062,\n",
              " 'valid_data': <torchtext.data.dataset.TabularDataset at 0x7f9532886470>,\n",
              " 'valid_iterator': <torchtext.data.iterator.BucketIterator at 0x7f952ad97128>,\n",
              " 'virtual_memory': <function psutil.virtual_memory>,\n",
              " 'writer': <torch.utils.tensorboard.writer.SummaryWriter at 0x7f952b894f28>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKMTM4k_Z6hW",
        "outputId": "846d1ec9-d921-4d30-efc0-d92226dbae03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "test_data"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.dataset.TabularDataset at 0x7f9583a2c278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBWfjEzCaSiC"
      },
      "source": [
        "vars(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwaF0FQTaaXr"
      },
      "source": [
        "test_data.__dict__ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2R-sfrxaCXM",
        "outputId": "881ce2fb-d1b2-4677-b003-e54dd69b6d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njjnOteyZ4Pc",
        "outputId": "5927c773-ed84-42f4-c654-2189b3d2aae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "test_data[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.example.Example at 0x7f952ccb1240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzw8mAYYZIDg",
        "outputId": "a836c755-ad45-451d-e390-a9780074b810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(vars(test_data[0]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['중', '장기', '적', '으로', 'cj', 'enm', '의', '기업', '가치', '가', '상승', '할', '것', '이', '란', '전망', '을', '내놓', '고', '있', '다', '.'], 'trg': ['it', 'is', 'predicting', 'that', 'cj', 'enm', \"'s\", 'corporate', 'value', 'will', 'rise', 'in', 'the', 'mid-', 'to', 'long', '-', 'term', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsUxn637ZIGT",
        "outputId": "c5bb2f8e-0c0c-42e7-96e7-c100da617a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "print(test_data[0].__dict__)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['중', '장기', '적', '으로', 'cj', 'enm', '의', '기업', '가치', '가', '상승', '할', '것', '이', '란', '전망', '을', '내놓', '고', '있', '다', '.'], 'trg': ['it', 'is', 'predicting', 'that', 'cj', 'enm', \"'s\", 'corporate', 'value', 'will', 'rise', 'in', 'the', 'mid-', 'to', 'long', '-', 'term', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xlr5ZOpcWu5"
      },
      "source": [
        "- source sentence (korean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B75CewqYZIAX",
        "outputId": "944991b9-a61a-4333-dac9-6dbdd1cff69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "src = vars(test_data[0])[\"src\"]\n",
        "\n",
        "print(src)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['중', '장기', '적', '으로', 'cj', 'enm', '의', '기업', '가치', '가', '상승', '할', '것', '이', '란', '전망', '을', '내놓', '고', '있', '다', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzWJox1Tccx3"
      },
      "source": [
        "- target sentence (english)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wha-xXzccTB-",
        "outputId": "e0ed04f0-be91-441a-9d87-80b72aab4cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "trg = vars(test_data[0])[\"trg\"]\n",
        "\n",
        "print(trg)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['it', 'is', 'predicting', 'that', 'cj', 'enm', \"'s\", 'corporate', 'value', 'will', 'rise', 'in', 'the', 'mid-', 'to', 'long', '-', 'term', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRlFt5png72p"
      },
      "source": [
        "# test the translate_sentence() function\n",
        "- 입력 문장을 list에서 string으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noCVsSpjieCW",
        "outputId": "84d43f6d-461d-408d-f629-fb5a3dc997a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "src_txt = ' '.join(src)\n",
        "src_txt = src_txt.replace(' .', '.')\n",
        "src_txt"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'중 장기 적 으로 cj enm 의 기업 가치 가 상승 할 것 이 란 전망 을 내놓 고 있 다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j58j7eUAcqnf",
        "outputId": "9c0e6c92-579e-4ccc-cded-5eb50332cccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "prediction = translate_sentence(model, src_txt, korean, english, device, max_length = 100)\n",
        "prediction = prediction[:-1]\n",
        "\n",
        "print(prediction)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'government', 'is', 'expected', 'to', 'be', 'the', 'company', 'to', 'be', 'expected', 'to', 'be', 'expected', 'to', 'the', 'future', 'of', 'the', 'future', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I_T-KaljSst"
      },
      "source": [
        "# BLEU score test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9QrdmtSY9Fr",
        "outputId": "2c6a1838-fdbc-4b38-e2f6-811d9d42447c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# bleu_score() function example\n",
        "\n",
        "candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
        "references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
        "bleu_score(candidate_corpus, references_corpus)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8408964276313782"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN-RCz33lQ4c"
      },
      "source": [
        "targets = []\n",
        "outputs = []\n",
        "\n",
        "targets.append([trg])\n",
        "outputs.append(prediction)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG0FwelYg66r",
        "outputId": "17331a8a-a23f-4c1a-8137-9a58516f8b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "print(targets)\n",
        "print(outputs)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['it', 'is', 'predicting', 'that', 'cj', 'enm', \"'s\", 'corporate', 'value', 'will', 'rise', 'in', 'the', 'mid-', 'to', 'long', '-', 'term', '.']]]\n",
            "[['the', 'government', 'is', 'expected', 'to', 'be', 'the', 'company', 'to', 'be', 'expected', 'to', 'be', 'expected', 'to', 'the', 'future', 'of', 'the', 'future', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUHsDqZKg7IU",
        "outputId": "d7a6c159-c179-4f93-b056-6d87f7f3f739",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "bleu_score(outputs, targets)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkq6nVcmvGEu"
      },
      "source": [
        "# Test the model again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFmWwQ2xg7Lp",
        "outputId": "9a4cb18c-03cd-428b-d7b4-c4007e9afe0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def bleu(data, model, korean, english, device):\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    for example in data:\n",
        "        src = vars(example)[\"src\"]\n",
        "        src = ' '.join(src).replace(' .', '.')\n",
        "        trg = vars(example)[\"trg\"]\n",
        "\n",
        "        prediction = translate_sentence(model, src, korean, english, device)\n",
        "        prediction = prediction[:-1]  # remove <eos> token\n",
        "\n",
        "        targets.append([trg])\n",
        "        outputs.append(prediction)\n",
        "\n",
        "    return bleu_score(outputs, targets)\n",
        "\n",
        "bleu(test_data, model, korean, english, device)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.018196161836385727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mevLJaQh4FVm"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "**translate_sentence()에서 에러 발생한 이유**\n",
        "- model training code에서 에러가 나지 않았던 이유는 입력 sentence가 토큰화된 리스트가 아닌, string 문자형이였기 때문\n",
        "- model test에서 사용되었던 bleu()함수에는 translate_sentence()함수가 들어있었는데, 입력 sentence가 토큰화된 리스트로 입력되어서 에러가 발생하였음\n",
        "- 독영병렬코퍼스로 학습을 시켰을 경우 list->list 번역이 가능했던 반면, 한영병렬코퍼스는 string->list 번역만 가능하였음\n",
        "- 어떤 차이점이 있는지 translate_sentence()함수를 분석해볼 필요가 있음"
      ]
    }
  ]
}